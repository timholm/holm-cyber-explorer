<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>D6-009 â€” File System Architecture for Longevity - holm.chat</title>
  <style>
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body { font-family: Georgia, serif; line-height: 1.6; color: #222; display: flex; min-height: 100vh; }

    /* Sidebar */
    .sidebar { width: 260px; min-width: 260px; background: #1a1a2e; color: #ccc; height: 100vh; position: fixed; top: 0; left: 0; overflow-y: auto; z-index: 100; transition: transform 0.2s; }
    .sidebar-header { padding: 1em; border-bottom: 1px solid #333; display: flex; justify-content: space-between; align-items: center; }
    .home-link { color: #fff; text-decoration: none; font-weight: bold; font-size: 1.1em; }
    .sidebar-toggle { background: none; border: none; color: #888; font-size: 1.4em; cursor: pointer; display: none; }
    .sidebar-content { padding: 0.5em 0; }
    .sidebar details { border-bottom: 1px solid #2a2a4a; }
    .sidebar summary { padding: 0.6em 1em; cursor: pointer; font-size: 0.85em; font-weight: bold; color: #aaa; text-transform: uppercase; letter-spacing: 0.03em; }
    .sidebar summary:hover { color: #fff; background: #2a2a4a; }
    .sidebar ul { list-style: none; padding: 0 0 0.4em 0; }
    .sidebar li { font-size: 0.82em; }
    .sidebar li a { display: block; padding: 0.3em 1em 0.3em 1.8em; color: #bbb; text-decoration: none; }
    .sidebar li a:hover { color: #fff; background: #2a2a4a; }
    .sidebar li.active a { color: #fff; background: #16213e; border-left: 3px solid #4a90d9; padding-left: calc(1.8em - 3px); }

    /* Main content */
    main { margin-left: 260px; flex: 1; max-width: 52em; padding: 2em 2em 4em 2em; }
    .mobile-menu-btn { display: none; position: fixed; top: 0.6em; left: 0.6em; z-index: 200; background: #1a1a2e; color: #fff; border: none; padding: 0.4em 0.7em; font-size: 1.2em; cursor: pointer; border-radius: 4px; }

    /* Typography */
    h1 { border-bottom: 2px solid #333; padding-bottom: 0.3em; margin-bottom: 0.8em; font-size: 1.6em; }
    h2 { border-bottom: 1px solid #ddd; padding-bottom: 0.2em; margin-top: 2em; margin-bottom: 0.6em; font-size: 1.25em; }
    h3 { margin-top: 1.5em; margin-bottom: 0.4em; }
    p { margin-bottom: 0.8em; }
    section { margin-bottom: 2em; }
    ul, ol { margin: 0.5em 0 0.8em 1.5em; }
    li { margin-bottom: 0.3em; }
    aside.metadata { background: #f8f8f8; border-left: 3px solid #666; padding: 0.8em 1.2em; margin: 1em 0; font-size: 0.9em; }
    aside.metadata dl { margin: 0; }
    aside.metadata dt { font-weight: bold; display: inline; }
    aside.metadata dt::after { content: ": "; }
    aside.metadata dd { display: inline; margin: 0; }
    aside.metadata dd::after { content: "\A"; white-space: pre; }
    table { border-collapse: collapse; width: 100%; margin: 1em 0; font-size: 0.9em; }
    th, td { border: 1px solid #ccc; padding: 0.5em; text-align: left; }
    th { background: #f0f0f0; }
    pre { background: #f5f5f5; padding: 1em; overflow-x: auto; border: 1px solid #ddd; margin: 0.8em 0; }
    code { font-family: "Courier New", monospace; font-size: 0.9em; }
    blockquote { border-left: 3px solid #999; margin: 0.8em 0; padding-left: 1em; color: #555; }

    /* Mobile */
    @media (max-width: 800px) {
        .sidebar { transform: translateX(-100%); }
        .sidebar-toggle { display: block; }
        body:not(.sidebar-closed) .sidebar { transform: translateX(-100%); }
        body.sidebar-open .sidebar { transform: translateX(0); }
        main { margin-left: 0; padding: 3em 1em 2em 1em; }
        .mobile-menu-btn { display: block; }
    }

  </style>
</head>
<body>
<button class="mobile-menu-btn" onclick="document.body.classList.toggle('sidebar-open')">&#9776;</button>
<nav class="sidebar" id="sidebar">
  <div class="sidebar-header">
    <a href="index.html" class="home-link">holm.chat</a>
    <button class="sidebar-toggle" onclick="document.body.classList.toggle('sidebar-closed')" aria-label="Toggle menu">&times;</button>
  </div>
  <div class="sidebar-content">
  <details>
    <summary>Constitution & Philosophy</summary>
    <ul>
      <li><a href="con-001.html">CON-001</a></li>
      <li><a href="con-002.html">CON-002</a></li>
      <li><a href="con-003.html">CON-003</a></li>
      <li><a href="con-004.html">CON-004</a></li>
      <li><a href="con-005.html">CON-005</a></li>
      <li><a href="con-006.html">CON-006</a></li>
      <li><a href="con-007.html">CON-007</a></li>
      <li><a href="con-008.html">CON-008</a></li>
      <li><a href="con-009.html">CON-009</a></li>
      <li><a href="con-010.html">CON-010</a></li>
      <li><a href="con-011.html">CON-011</a></li>
      <li><a href="con-012.html">CON-012</a></li>
      <li><a href="con-013.html">CON-013</a></li>
      <li><a href="domain-1.html">DOMAIN-1</a></li>
      <li><a href="eth-001.html">ETH-001</a></li>
    </ul>
  </details>
  <details>
    <summary>Governance & Authority</summary>
    <ul>
      <li><a href="domain-2.html">DOMAIN-2</a></li>
      <li><a href="gov-001.html">GOV-001</a></li>
      <li><a href="gov-002.html">GOV-002</a></li>
      <li><a href="gov-003.html">GOV-003</a></li>
      <li><a href="gov-004.html">GOV-004</a></li>
      <li><a href="gov-005.html">GOV-005</a></li>
      <li><a href="gov-006.html">GOV-006</a></li>
      <li><a href="gov-007.html">GOV-007</a></li>
      <li><a href="gov-008.html">GOV-008</a></li>
      <li><a href="gov-009.html">GOV-009</a></li>
      <li><a href="gov-010.html">GOV-010</a></li>
      <li><a href="gov-011.html">GOV-011</a></li>
      <li><a href="gov-012.html">GOV-012</a></li>
      <li><a href="gov-013.html">GOV-013</a></li>
      <li><a href="gov-014.html">GOV-014</a></li>
    </ul>
  </details>
  <details>
    <summary>Security & Integrity</summary>
    <ul>
      <li><a href="domain-3.html">DOMAIN-3</a></li>
      <li><a href="sec-001.html">SEC-001</a></li>
      <li><a href="sec-002.html">SEC-002</a></li>
      <li><a href="sec-003.html">SEC-003</a></li>
      <li><a href="sec-004.html">SEC-004</a></li>
      <li><a href="sec-005.html">SEC-005</a></li>
      <li><a href="sec-006.html">SEC-006</a></li>
      <li><a href="sec-007.html">SEC-007</a></li>
      <li><a href="sec-008.html">SEC-008</a></li>
      <li><a href="sec-009.html">SEC-009</a></li>
      <li><a href="sec-010.html">SEC-010</a></li>
      <li><a href="sec-011.html">SEC-011</a></li>
      <li><a href="sec-012.html">SEC-012</a></li>
      <li><a href="sec-013.html">SEC-013</a></li>
      <li><a href="sec-014.html">SEC-014</a></li>
    </ul>
  </details>
  <details>
    <summary>Infrastructure & Power</summary>
    <ul>
      <li><a href="d4-001.html">D4-001</a></li>
      <li><a href="d4-002.html">D4-002</a></li>
      <li><a href="d4-003.html">D4-003</a></li>
      <li><a href="d4-004.html">D4-004</a></li>
      <li><a href="d4-005.html">D4-005</a></li>
      <li><a href="d4-006.html">D4-006</a></li>
      <li><a href="d4-007.html">D4-007</a></li>
      <li><a href="d4-008.html">D4-008</a></li>
      <li><a href="d4-009.html">D4-009</a></li>
      <li><a href="d4-010.html">D4-010</a></li>
      <li><a href="d4-011.html">D4-011</a></li>
      <li><a href="d4-012.html">D4-012</a></li>
      <li><a href="d4-013.html">D4-013</a></li>
      <li><a href="d4-014.html">D4-014</a></li>
      <li><a href="domain-4.html">DOMAIN-4</a></li>
    </ul>
  </details>
  <details>
    <summary>Platform & Core Systems</summary>
    <ul>
      <li><a href="d5-001.html">D5-001</a></li>
      <li><a href="d5-002.html">D5-002</a></li>
      <li><a href="d5-003.html">D5-003</a></li>
      <li><a href="d5-004.html">D5-004</a></li>
      <li><a href="d5-005.html">D5-005</a></li>
      <li><a href="d5-006.html">D5-006</a></li>
      <li><a href="domain-5.html">DOMAIN-5</a></li>
    </ul>
  </details>
  <details open>
    <summary>Data & Archives</summary>
    <ul>
      <li><a href="d6-001.html">D6-001</a></li>
      <li><a href="d6-002.html">D6-002</a></li>
      <li><a href="d6-003.html">D6-003</a></li>
      <li><a href="d6-004.html">D6-004</a></li>
      <li><a href="d6-005.html">D6-005</a></li>
      <li><a href="d6-006.html">D6-006</a></li>
      <li><a href="d6-007.html">D6-007</a></li>
      <li><a href="d6-008.html">D6-008</a></li>
      <li class="active"><a href="d6-009.html">D6-009</a></li>
      <li><a href="d6-010.html">D6-010</a></li>
      <li><a href="d6-011.html">D6-011</a></li>
      <li><a href="d6-012.html">D6-012</a></li>
      <li><a href="d6-013.html">D6-013</a></li>
      <li><a href="d6-014.html">D6-014</a></li>
      <li><a href="d6-015.html">D6-015</a></li>
      <li><a href="domain-6.html">DOMAIN-6</a></li>
    </ul>
  </details>
  <details>
    <summary>Intelligence & Analysis</summary>
    <ul>
      <li><a href="d7-001.html">D7-001</a></li>
      <li><a href="d7-002.html">D7-002</a></li>
      <li><a href="d7-003.html">D7-003</a></li>
      <li><a href="d7-004.html">D7-004</a></li>
      <li><a href="d7-005.html">D7-005</a></li>
      <li><a href="d7-006.html">D7-006</a></li>
      <li><a href="d7-007.html">D7-007</a></li>
      <li><a href="d7-008.html">D7-008</a></li>
      <li><a href="d7-009.html">D7-009</a></li>
      <li><a href="d7-010.html">D7-010</a></li>
      <li><a href="d7-011.html">D7-011</a></li>
      <li><a href="d7-012.html">D7-012</a></li>
      <li><a href="d7-013.html">D7-013</a></li>
      <li><a href="domain-7.html">DOMAIN-7</a></li>
    </ul>
  </details>
  <details>
    <summary>Automation & Agents</summary>
    <ul>
      <li><a href="d8-001.html">D8-001</a></li>
      <li><a href="d8-002.html">D8-002</a></li>
      <li><a href="d8-003.html">D8-003</a></li>
      <li><a href="d8-004.html">D8-004</a></li>
      <li><a href="d8-005.html">D8-005</a></li>
      <li><a href="d8-006.html">D8-006</a></li>
      <li><a href="d8-007.html">D8-007</a></li>
      <li><a href="d8-008.html">D8-008</a></li>
      <li><a href="d8-009.html">D8-009</a></li>
      <li><a href="d8-010.html">D8-010</a></li>
      <li><a href="d8-011.html">D8-011</a></li>
      <li><a href="d8-012.html">D8-012</a></li>
      <li><a href="d8-013.html">D8-013</a></li>
      <li><a href="domain-8.html">DOMAIN-8</a></li>
    </ul>
  </details>
  <details>
    <summary>Education & Training</summary>
    <ul>
      <li><a href="d9-001.html">D9-001</a></li>
      <li><a href="d9-002.html">D9-002</a></li>
      <li><a href="d9-003.html">D9-003</a></li>
      <li><a href="d9-004.html">D9-004</a></li>
      <li><a href="d9-005.html">D9-005</a></li>
      <li><a href="d9-006.html">D9-006</a></li>
      <li><a href="d9-007.html">D9-007</a></li>
      <li><a href="d9-008.html">D9-008</a></li>
      <li><a href="d9-009.html">D9-009</a></li>
      <li><a href="d9-010.html">D9-010</a></li>
      <li><a href="d9-011.html">D9-011</a></li>
      <li><a href="d9-012.html">D9-012</a></li>
      <li><a href="d9-013.html">D9-013</a></li>
      <li><a href="domain-9.html">DOMAIN-9</a></li>
    </ul>
  </details>
  <details>
    <summary>User Operations</summary>
    <ul>
      <li><a href="d10-002.html">D10-002</a></li>
      <li><a href="d10-003.html">D10-003</a></li>
      <li><a href="d10-004.html">D10-004</a></li>
      <li><a href="d10-005.html">D10-005</a></li>
      <li><a href="d10-006.html">D10-006</a></li>
      <li><a href="d10-007.html">D10-007</a></li>
      <li><a href="d10-008.html">D10-008</a></li>
      <li><a href="d10-009.html">D10-009</a></li>
      <li><a href="d10-010.html">D10-010</a></li>
      <li><a href="d10-011.html">D10-011</a></li>
      <li><a href="d10-012.html">D10-012</a></li>
      <li><a href="d10-013.html">D10-013</a></li>
      <li><a href="d10-014.html">D10-014</a></li>
      <li><a href="domain-10.html">DOMAIN-10</a></li>
      <li><a href="ops-001.html">OPS-001</a></li>
    </ul>
  </details>
  <details>
    <summary>Administration</summary>
    <ul>
      <li><a href="d11-001.html">D11-001</a></li>
      <li><a href="d11-002.html">D11-002</a></li>
      <li><a href="d11-003.html">D11-003</a></li>
      <li><a href="d11-004.html">D11-004</a></li>
      <li><a href="d11-005.html">D11-005</a></li>
      <li><a href="d11-006.html">D11-006</a></li>
      <li><a href="d11-007.html">D11-007</a></li>
      <li><a href="d11-008.html">D11-008</a></li>
      <li><a href="d11-009.html">D11-009</a></li>
      <li><a href="d11-010.html">D11-010</a></li>
      <li><a href="d11-011.html">D11-011</a></li>
      <li><a href="d11-012.html">D11-012</a></li>
      <li><a href="d11-013.html">D11-013</a></li>
      <li><a href="domain-11.html">DOMAIN-11</a></li>
    </ul>
  </details>
  <details>
    <summary>Disaster Recovery</summary>
    <ul>
      <li><a href="d12-001.html">D12-001</a></li>
      <li><a href="d12-002.html">D12-002</a></li>
      <li><a href="d12-003.html">D12-003</a></li>
      <li><a href="d12-004.html">D12-004</a></li>
      <li><a href="d12-005.html">D12-005</a></li>
      <li><a href="d12-006.html">D12-006</a></li>
      <li><a href="d12-007.html">D12-007</a></li>
      <li><a href="d12-008.html">D12-008</a></li>
      <li><a href="d12-009.html">D12-009</a></li>
      <li><a href="d12-010.html">D12-010</a></li>
      <li><a href="d12-011.html">D12-011</a></li>
      <li><a href="d12-012.html">D12-012</a></li>
      <li><a href="d12-013.html">D12-013</a></li>
      <li><a href="d12-014.html">D12-014</a></li>
      <li><a href="domain-12.html">DOMAIN-12</a></li>
    </ul>
  </details>
  <details>
    <summary>Evolution & Adaptation</summary>
    <ul>
      <li><a href="d13-001.html">D13-001</a></li>
      <li><a href="d13-002.html">D13-002</a></li>
      <li><a href="d13-003.html">D13-003</a></li>
      <li><a href="d13-004.html">D13-004</a></li>
      <li><a href="d13-005.html">D13-005</a></li>
      <li><a href="d13-006.html">D13-006</a></li>
      <li><a href="d13-007.html">D13-007</a></li>
      <li><a href="d13-008.html">D13-008</a></li>
      <li><a href="d13-009.html">D13-009</a></li>
      <li><a href="d13-010.html">D13-010</a></li>
      <li><a href="d13-011.html">D13-011</a></li>
      <li><a href="d13-012.html">D13-012</a></li>
      <li><a href="d13-013.html">D13-013</a></li>
      <li><a href="domain-13.html">DOMAIN-13</a></li>
    </ul>
  </details>
  <details>
    <summary>Research & Theory</summary>
    <ul>
      <li><a href="d14-001.html">D14-001</a></li>
      <li><a href="d14-002.html">D14-002</a></li>
      <li><a href="d14-003.html">D14-003</a></li>
      <li><a href="d14-004.html">D14-004</a></li>
      <li><a href="d14-005.html">D14-005</a></li>
      <li><a href="d14-006.html">D14-006</a></li>
      <li><a href="d14-007.html">D14-007</a></li>
      <li><a href="d14-008.html">D14-008</a></li>
      <li><a href="d14-009.html">D14-009</a></li>
      <li><a href="d14-010.html">D14-010</a></li>
      <li><a href="d14-011.html">D14-011</a></li>
      <li><a href="d14-012.html">D14-012</a></li>
      <li><a href="d14-013.html">D14-013</a></li>
      <li><a href="domain-14.html">DOMAIN-14</a></li>
    </ul>
  </details>
  <details>
    <summary>Ethics & Safeguards</summary>
    <ul>
      <li><a href="d15-001.html">D15-001</a></li>
      <li><a href="d15-002.html">D15-002</a></li>
      <li><a href="d15-003.html">D15-003</a></li>
      <li><a href="d15-004.html">D15-004</a></li>
      <li><a href="d15-005.html">D15-005</a></li>
      <li><a href="domain-15.html">DOMAIN-15</a></li>
    </ul>
  </details>
  <details>
    <summary>Interface & Navigation</summary>
    <ul>
      <li><a href="d16-001.html">D16-001</a></li>
      <li><a href="d16-002.html">D16-002</a></li>
      <li><a href="d16-003.html">D16-003</a></li>
      <li><a href="d16-004.html">D16-004</a></li>
      <li><a href="d16-005.html">D16-005</a></li>
      <li><a href="d16-006.html">D16-006</a></li>
      <li><a href="d16-007.html">D16-007</a></li>
      <li><a href="d16-008.html">D16-008</a></li>
      <li><a href="d16-009.html">D16-009</a></li>
      <li><a href="d16-010.html">D16-010</a></li>
      <li><a href="d16-011.html">D16-011</a></li>
      <li><a href="d16-012.html">D16-012</a></li>
      <li><a href="d16-013.html">D16-013</a></li>
      <li><a href="d16-014.html">D16-014</a></li>
    </ul>
  </details>
  <details>
    <summary>Scaling & Federation</summary>
    <ul>
      <li><a href="d17-001.html">D17-001</a></li>
      <li><a href="d17-002.html">D17-002</a></li>
      <li><a href="d17-003.html">D17-003</a></li>
      <li><a href="d17-004.html">D17-004</a></li>
      <li><a href="d17-005.html">D17-005</a></li>
      <li><a href="d17-006.html">D17-006</a></li>
      <li><a href="d17-007.html">D17-007</a></li>
      <li><a href="d17-008.html">D17-008</a></li>
      <li><a href="d17-009.html">D17-009</a></li>
      <li><a href="d17-010.html">D17-010</a></li>
      <li><a href="d17-011.html">D17-011</a></li>
      <li><a href="d17-012.html">D17-012</a></li>
      <li><a href="d17-013.html">D17-013</a></li>
      <li><a href="d17-014.html">D17-014</a></li>
      <li><a href="d17-015.html">D17-015</a></li>
    </ul>
  </details>
  <details>
    <summary>Import & Quarantine</summary>
    <ul>
      <li><a href="d18-001.html">D18-001</a></li>
      <li><a href="d18-002.html">D18-002</a></li>
      <li><a href="d18-003.html">D18-003</a></li>
      <li><a href="d18-004.html">D18-004</a></li>
      <li><a href="d18-005.html">D18-005</a></li>
      <li><a href="d18-006.html">D18-006</a></li>
      <li><a href="d18-007.html">D18-007</a></li>
      <li><a href="d18-008.html">D18-008</a></li>
      <li><a href="d18-009.html">D18-009</a></li>
      <li><a href="d18-010.html">D18-010</a></li>
      <li><a href="d18-011.html">D18-011</a></li>
      <li><a href="d18-012.html">D18-012</a></li>
      <li><a href="d18-013.html">D18-013</a></li>
      <li><a href="d18-014.html">D18-014</a></li>
    </ul>
  </details>
  <details>
    <summary>Quality Assurance</summary>
    <ul>
      <li><a href="d19-001.html">D19-001</a></li>
      <li><a href="d19-002.html">D19-002</a></li>
      <li><a href="d19-003.html">D19-003</a></li>
      <li><a href="d19-004.html">D19-004</a></li>
      <li><a href="d19-005.html">D19-005</a></li>
      <li><a href="d19-006.html">D19-006</a></li>
      <li><a href="d19-007.html">D19-007</a></li>
      <li><a href="d19-008.html">D19-008</a></li>
      <li><a href="d19-009.html">D19-009</a></li>
      <li><a href="d19-010.html">D19-010</a></li>
      <li><a href="d19-011.html">D19-011</a></li>
      <li><a href="d19-012.html">D19-012</a></li>
      <li><a href="d19-013.html">D19-013</a></li>
      <li><a href="d19-014.html">D19-014</a></li>
      <li><a href="d19-015.html">D19-015</a></li>
    </ul>
  </details>
  <details>
    <summary>Institutional Memory</summary>
    <ul>
      <li><a href="d20-001.html">D20-001</a></li>
      <li><a href="d20-002.html">D20-002</a></li>
      <li><a href="d20-003.html">D20-003</a></li>
      <li><a href="d20-004.html">D20-004</a></li>
      <li><a href="d20-005.html">D20-005</a></li>
      <li><a href="d20-006.html">D20-006</a></li>
      <li><a href="d20-007.html">D20-007</a></li>
      <li><a href="d20-008.html">D20-008</a></li>
      <li><a href="d20-009.html">D20-009</a></li>
      <li><a href="d20-010.html">D20-010</a></li>
      <li><a href="d20-011.html">D20-011</a></li>
      <li><a href="d20-012.html">D20-012</a></li>
      <li><a href="d20-013.html">D20-013</a></li>
    </ul>
  </details>
  <details>
    <summary>Meta-Documentation</summary>
    <ul>
      <li><a href="meta-001.html">META-001</a></li>
      <li><a href="meta-002.html">META-002</a></li>
      <li><a href="meta-003.html">META-003</a></li>
      <li><a href="meta-004.html">META-004</a></li>
      <li><a href="meta-005.html">META-005</a></li>
      <li><a href="meta-006.html">META-006</a></li>
      <li><a href="meta-007.html">META-007</a></li>
      <li><a href="meta-008.html">META-008</a></li>
      <li><a href="meta-009.html">META-009</a></li>
      <li><a href="meta-010.html">META-010</a></li>
      <li><a href="meta-011.html">META-011</a></li>
      <li><a href="meta-012.html">META-012</a></li>
      <li><a href="meta-013.html">META-013</a></li>
      <li><a href="meta-014.html">META-014</a></li>
      <li><a href="meta-015.html">META-015</a></li>
      <li><a href="meta-framework.html">Unifying Standards for the Hol</a></li>
    </ul>
  </details>
  <details>
    <summary>Framework</summary>
    <ul>
      <li><a href="appendix-a.html">ARTICLE INDEX (ALL DOMAINS)</a></li>
      <li><a href="appendix-a-2.html">ARTICLE TEMPLATE (FULL)</a></li>
      <li><a href="appendix-b.html">DOCUMENT VERSIONING SCHEME</a></li>
      <li><a href="appendix-b-2.html">META-RULES FOR ALL DOCUMENTATI</a></li>
      <li><a href="appendix-c.html">HOW TO USE THIS FRAMEWORK</a></li>
      <li><a href="consolidated-writing-schedule.html">Consolidated Writing Schedule</a></li>
      <li><a href="cross-domain-integration.html">CROSS-DOMAIN INTEGRATION</a></li>
      <li><a href="cross-domain-integration-2.html">CROSS-DOMAIN INTEGRATION</a></li>
      <li><a href="cross-domain-dependency-matrix.html">Cross-Domain Dependency Matrix</a></li>
      <li><a href="cross-domain-synthesis-domains-6-10.html">CROSS-DOMAIN SYNTHESIS: DOMAIN</a></li>
      <li><a href="universal-review-process.html">UNIVERSAL REVIEW PROCESS</a></li>
      <li><a href="universal-maintenance-plan.html">UNIVERSAL MAINTENANCE PLAN</a></li>
    </ul>
  </details>
  </div>
</nav>

<main>
<article id="d6-009">
  <h1>D6-009 &mdash; File System Architecture for Longevity</h1>
<aside class="metadata">
<dl>
  <dt>Document ID</dt>
  <dd>D6-009</dd>
  <dt>Domain</dt>
  <dd>6 -- Data & Archives</dd>
  <dt>Version</dt>
  <dd>1.0.0</dd>
  <dt>Date</dt>
  <dd>2026-02-16</dd>
  <dt>Status</dt>
  <dd>Ratified</dd>
  <dt>Depends On</dt>
  <dd>ETH-001, CON-001, SEC-001, OPS-001, D6-001, D6-005, D6-006, D6-007</dd>
  <dt>Depended Upon By</dt>
  <dd>D6-010, D6-011, D6-013, all Domain 5 articles involving storage configuration, all Domain 12 disaster recovery articles involving filesystem repair.</dd>
</dl>
</aside>

<section>
<h2>Purpose</h2>

<p>This article defines how the institution selects, configures, and maintains the filesystems that underpin all data storage. A filesystem is the layer between the physical storage media and the data that lives on it. It determines how data is organized on disk, how integrity is maintained, how failures are detected and survived, and how storage can grow or shrink over time. A poor filesystem choice can silently corrupt data. A good filesystem choice, properly maintained, can detect and repair corruption before data is lost.</p>
<p>The filesystem is the single most consequential software decision in the institution's data architecture. The operating system can be reinstalled. Applications can be replaced. But the filesystem is the structure that holds the data itself. Changing it requires moving every byte of institutional data off the old filesystem, reformatting the storage, and moving every byte back. This is not a decision to revisit casually or frequently. It must be made correctly, documented thoroughly, and maintained relentlessly.</p>
<p>This article evaluates the major filesystem options available for Linux-based air-gapped systems as of the founding date. It establishes the criteria by which filesystems are judged, makes a specific recommendation for the institution, defines the maintenance schedules that keep the chosen filesystem healthy, and provides the migration framework for when the chosen filesystem must eventually be replaced.</p>

</section>
<section>
<h2>Scope</h2>

<p>This article covers:</p>
<ul>
<li>The criteria for evaluating filesystems for multi-decade institutional use.</li>
<li>Detailed analysis of ZFS, Btrfs, and ext4 as candidates.</li>
<li>The specific filesystem configuration chosen for this institution, with rationale.</li>
<li>Scrub schedules, snapshot strategies, and pool management procedures.</li>
<li>Monitoring and alerting for filesystem health.</li>
<li>The migration path when filesystem technology changes.</li>
</ul>
<p>This article does not cover:</p>
<ul>
<li>Physical media selection (see D6-005 and D6-011).</li>
<li>Backup procedures (see D6-006; this article covers the filesystem that backups live on, not the backup process itself).</li>
<li>File format selection (see D6-010).</li>
<li>Logical directory structure and data organization (see D6-004).</li>
<li>Encryption at rest (see SEC-003; this article notes where filesystem-level encryption intersects with filesystem choice but does not define encryption policy).</li>
</ul>

</section>
<section>
<h2>Background</h2>

<h3>3.1 What a Filesystem Actually Does</h3>
<p>A filesystem performs four functions that matter for longevity. First, it maps logical file names and directory structures to physical locations on storage media. Second, it manages the allocation of storage space -- tracking which blocks are in use, which are free, and how to find them efficiently. Third, it maintains metadata about files -- ownership, permissions, timestamps, and size. Fourth, and most critically for this institution, it either protects data integrity or does not.</p>
<p>Traditional filesystems like ext4 store data and trust that the underlying hardware returns what was written. They do not independently verify that the bits on disk still match what was originally written. If a disk sector degrades silently -- a phenomenon called bit rot -- the filesystem will happily return the corrupted data without any indication that something is wrong. You will not know your data is damaged until you try to use it and discover the corruption, which may be months or years after the corruption occurred.</p>
<p>Modern copy-on-write filesystems like ZFS and Btrfs take a fundamentally different approach. They store checksums alongside data. Every time data is read, the checksum is verified. If corruption is detected, the filesystem can report it immediately and, if configured with redundancy, repair it automatically from a good copy. This is not a minor feature. For an institution that must keep data intact for fifty years across multiple hardware generations, filesystem-level integrity checking is the difference between detecting corruption when it happens and discovering it when the data is needed and it is too late.</p>
<h3>3.2 The Air-Gap Implication</h3>
<p>In a connected environment, many filesystem management tasks can be automated and monitored remotely. Storage pools can be expanded by ordering cloud capacity. Health alerts can be routed to monitoring services. In this institution, every filesystem operation happens locally, is performed by the operator, and is monitored by the operator. The filesystem must therefore be manageable by a single person with standard command-line tools, must produce clear and interpretable health reports, and must fail in ways that are obvious rather than silent.</p>
<h3>3.3 The Fifty-Year Problem</h3>
<p>No filesystem in use today has existed for fifty years. ext4 dates to 2008. ZFS dates to 2005. Btrfs dates to 2009. None of them have a fifty-year track record because none of them are fifty years old. The institution must therefore choose a filesystem based on design principles, community health, and institutional maturity rather than a proven half-century of operation. It must also plan for the near-certainty that the chosen filesystem will eventually be superseded and all data will need to migrate to its successor.</p>

</section>
<section>
<h2>System Model</h2>

<h3>4.1 Evaluation Criteria</h3>
<p>The institution evaluates filesystems against seven criteria, ranked by importance:</p>
<p><strong>Criterion 1: Data Integrity Verification.</strong> Does the filesystem checksum data at rest and verify checksums on read? Can it detect silent corruption? Can it repair detected corruption from redundant copies? This is the most important criterion. A filesystem without integrity verification is unsuitable for institutional use regardless of its other qualities.</p>
<p><strong>Criterion 2: Proven Stability.</strong> How long has the filesystem been in production use? How large is the user base? How frequently are data-loss bugs discovered? A filesystem with a fifteen-year track record of stable operation on production systems is preferred over one with superior theoretical design but a shorter or rockier history.</p>
<p><strong>Criterion 3: Redundancy and Self-Healing.</strong> Can the filesystem maintain multiple copies of data and automatically repair corruption from good copies? This is distinct from backup -- it is the filesystem's ability to maintain internal redundancy and use it transparently.</p>
<p><strong>Criterion 4: Snapshot Capability.</strong> Can the filesystem create point-in-time snapshots efficiently? Snapshots are critical for rollback after failed operations, for creating consistent backup points, and for maintaining historical states of the data without duplicating storage.</p>
<p><strong>Criterion 5: Manageable Complexity.</strong> Can a single operator understand, configure, maintain, and troubleshoot the filesystem without specialized training? Are the command-line tools well-documented? Are error messages clear? Is the community knowledge base sufficient that problems can be solved from archived documentation?</p>
<p><strong>Criterion 6: Hardware Flexibility.</strong> Does the filesystem work well with the types of storage media the institution uses? Can it span multiple disks? Can disks be added or replaced without reformatting? Does it handle mixed-size disks gracefully?</p>
<p><strong>Criterion 7: Licensing and Sovereignty.</strong> Is the filesystem open-source with a license that guarantees the institution can use, modify, and distribute it without external permission? Are there legal encumbrances that could affect availability?</p>
<h3>4.2 Candidate Analysis</h3>
<p><strong>ZFS (OpenZFS).</strong></p>
<p>ZFS was designed from the ground up as an integrated volume manager and filesystem with end-to-end data integrity as its core design goal. It checksums all data and metadata using SHA-256 or similar algorithms. It supports mirror and RAIDZ configurations that allow automatic self-healing when corruption is detected. Its snapshot mechanism is mature, efficient, and heavily tested. Its scrub mechanism -- a scheduled full-read verification of all data -- is the gold standard for proactive corruption detection.</p>
<p>Strengths for this institution: Best-in-class data integrity. Proven stability across two decades of production use in enterprise environments, including storage appliances, NAS systems, and mission-critical servers. Excellent snapshot and clone support. The scrub mechanism is exactly what a longevity-focused institution needs. Send/receive functionality allows efficient transfer of snapshots between pools, which is valuable for backup workflows.</p>
<p>Weaknesses for this institution: ZFS has a CDDL license, which creates friction with the Linux kernel's GPL license. It is not included in the mainline Linux kernel and must be installed as a separate module. This adds a maintenance burden -- kernel updates may require ZFS module rebuilds. The community has maintained compatibility reliably, but this is an ongoing dependency. ZFS pools cannot be easily shrunk -- you can add disks but removing them is limited. Memory requirements are higher than simpler filesystems, with a recommended minimum of 1 GB of RAM per terabyte of storage for optimal performance, and more for deduplication (which this institution should not use -- see Section 5).</p>
<p><strong>Btrfs.</strong></p>
<p>Btrfs was designed as a modern Linux-native copy-on-write filesystem with checksumming, snapshots, and integrated volume management. It is included in the mainline Linux kernel, eliminating the licensing and kernel-compatibility concerns of ZFS. It has been in development since 2009 and has been the default filesystem for some Linux distributions.</p>
<p>Strengths for this institution: Mainline kernel inclusion means no separate module maintenance. Good snapshot support. Checksumming of data and metadata. Subvolume architecture provides flexible data organization. Can add and remove devices from a filesystem while it is mounted. Lower memory overhead than ZFS.</p>
<p>Weaknesses for this institution: Btrfs has a more troubled stability history than ZFS. Its RAID5 and RAID6 implementations have been considered unreliable for years, with a well-documented write-hole problem. While RAID1 (mirror) is stable, this limits redundancy options. Some enterprise users (notably Red Hat) removed Btrfs support from their distributions, citing stability concerns, before partially reversing that decision. The community is smaller than ZFS's, and institutional knowledge is less deep. Scrub performance and reliability, while improving, has historically lagged behind ZFS. For an institution that must trust its filesystem with fifty years of irreplaceable data, Btrfs's stability history is a significant concern.</p>
<p><strong>ext4.</strong></p>
<p>ext4 is the default filesystem for most Linux distributions. It is mature, stable, extremely well-understood, and has the largest user base of any Linux filesystem. It has been in production since 2008 and its predecessor, ext3, since 2001.</p>
<p>Strengths for this institution: Maximum stability. Maximum compatibility. Every Linux tool, every recovery utility, every diagnostic program works with ext4. If the filesystem is damaged, the probability of finding tools and documentation to repair it is higher than for any other Linux filesystem. It is simple, well-understood, and predictable.</p>
<p>Weaknesses for this institution: ext4 does not checksum data. It checksums metadata (journal checksumming, added later), but data blocks can silently corrupt without detection. It does not support snapshots natively (snapshots require LVM, which adds another layer of complexity and another potential failure point). It does not support integrated redundancy -- RAID must be implemented at a separate layer (mdadm or hardware RAID), creating a split between the filesystem and the redundancy mechanism that complicates management and reduces the ability to do self-healing repair. For an institution where data integrity is the paramount concern, ext4's lack of data checksumming is a disqualifying weakness for primary storage.</p>
<h3>4.3 The Institutional Decision</h3>
<p><strong>Primary storage: ZFS.</strong> The institution uses ZFS (OpenZFS) for all primary data storage. The data integrity guarantees, proven stability, mature scrub mechanism, and excellent snapshot support make it the strongest choice for multi-decade data survival. The licensing friction with the Linux kernel is a real cost, accepted because the alternative -- trusting unchecksummed data for fifty years -- is an unacceptable risk.</p>
<p><strong>Backup and emergency storage: ext4.</strong> The institution maintains the ability to create and read ext4 filesystems for backup media, emergency recovery, and interoperability. If ZFS becomes unavailable due to a kernel incompatibility or community collapse, ext4 provides a fallback that can be read by any Linux system. Critical backups are stored on ext4-formatted media in addition to ZFS, ensuring that data is recoverable even if ZFS tools are lost.</p>
<p><strong>Btrfs: Not selected.</strong> Btrfs is not selected for institutional use at founding. Its stability history does not yet meet the threshold required by Criterion 2. This decision should be revisited at each five-year review. If Btrfs achieves the stability and community maturity of ZFS, it may become the preferred choice due to its mainline kernel inclusion.</p>
<h3>4.4 ZFS Configuration Standards</h3>
<p><strong>Pool Layout.</strong> The institution uses mirrored vdevs (mirror or RAIDZ2 for larger arrays) rather than striping. Mirrored vdevs provide the best resilience and the simplest recovery path. A mirror can be read by extracting a single disk -- no special tools are needed to access the raw data on one half of a mirror. RAIDZ2 (dual parity) is used when four or more disks are available and storage efficiency matters. RAIDZ1 (single parity) is not used -- a single-parity configuration cannot survive the combination of a disk failure and a read error during resilver, which becomes increasingly likely as disk sizes grow.</p>
<p><strong>Recordsize.</strong> The default recordsize of 128K is used for general storage. For databases or other workloads with small random I/O patterns, the recordsize may be tuned per dataset. The recordsize decision is documented per dataset.</p>
<p><strong>Compression.</strong> LZ4 compression is enabled by default on all datasets. LZ4 is fast enough that compression typically improves performance (less data to read from disk) and it meaningfully extends storage capacity. The compression algorithm is set per dataset, not per pool, allowing future changes without affecting existing data.</p>
<p><strong>Copies.</strong> For Tier 1 (Institutional Memory) data, the ZFS <code>copies=2</code> property is set, instructing ZFS to store two copies of every block within the same pool. This provides protection against single-block corruption even within a non-redundant pool. It is not a substitute for mirrored vdevs but an additional layer of defense for the most critical data.</p>
<p><strong>Checksumming.</strong> SHA-256 checksumming is used for all datasets. The default fletcher4 checksum is faster but SHA-256 provides cryptographic-strength collision resistance, which matters when data integrity is the primary design goal.</p>
<p><strong>Ashift.</strong> The ashift value is set correctly for the physical sector size of the disks in use. For modern drives with 4K physical sectors (common as of founding), ashift=12. Incorrect ashift degrades performance and, in some configurations, can reduce data safety. This value is set at pool creation and cannot be changed afterward.</p>
<h3>4.5 Scrub Schedule</h3>
<p>A scrub is a complete read of every block in a ZFS pool, with checksum verification. It is the primary mechanism for detecting silent corruption before it causes data loss.</p>
<p><strong>Schedule:</strong> Every pool is scrubbed at least once per month. Tier 1 data pools are scrubbed weekly. Scrubs are scheduled during periods of low system activity to minimize performance impact but are never deferred beyond their scheduled window. A missed scrub is an incident, documented in the operational log.</p>
<p><strong>Duration monitoring:</strong> Scrub duration is recorded each time. A significant increase in scrub duration (greater than 25% compared to baseline) indicates potential hardware degradation and triggers a hardware health investigation per D6-005.</p>
<p><strong>Error response:</strong> Any scrub that reports checksum errors triggers immediate investigation. If errors are repairable (ZFS repaired them from a redundant copy), the event is logged and the disk exhibiting errors is placed on watch. If errors are unrepairable, the affected data is identified, restored from backup if possible, and the disk is scheduled for replacement.</p>
<h3>4.6 Snapshot Strategy</h3>
<p><strong>Automated snapshots:</strong> ZFS snapshots are created on a tiered schedule:
- Hourly snapshots of active-use datasets, retained for 48 hours.
- Daily snapshots of all datasets, retained for 30 days.
- Weekly snapshots of all datasets, retained for 12 weeks.
- Monthly snapshots of all datasets, retained for 24 months.
- Annual snapshots of Tier 1 datasets, retained permanently.</p>
<p><strong>Naming convention:</strong> Snapshots are named <code>@auto-[frequency]-[YYYY-MM-DD-HHMM]</code> for automated snapshots and <code>@manual-[description]-[YYYY-MM-DD]</code> for manually created snapshots. Consistent naming enables automated cleanup and manual identification.</p>
<p><strong>Snapshot verification:</strong> At least one snapshot per quarter is selected for restore testing. The snapshot is cloned to a temporary dataset and the data is verified against known-good checksums. This confirms that the snapshot mechanism is functioning correctly and that snapshots are usable for recovery.</p>

</section>
<section>
<h2>Rules &amp; Constraints</h2>

<ul>
<li><strong>R-D6-09-01:</strong> All primary data storage must use a filesystem with data checksumming. Filesystems that do not checksum data at rest are prohibited for primary storage of Tier 1 or Tier 2 data.</li>
<li><strong>R-D6-09-02:</strong> ZFS deduplication must not be enabled. Deduplication consumes enormous amounts of RAM (approximately 5 GB per TB of deduplicated data), introduces fragmentation, and creates a single point of failure in the deduplication table. The storage savings do not justify the risk or the resource cost in this institution.</li>
<li><strong>R-D6-09-03:</strong> Every ZFS pool must be scrubbed at least monthly. Scrub results must be reviewed within 24 hours of completion. Unrepairable errors must be treated as incidents.</li>
<li><strong>R-D6-09-04:</strong> ZFS pool configuration changes (adding vdevs, replacing disks, changing properties) must be documented in the operational log before execution, including the exact commands to be run and the expected outcome. ZFS pool operations are not reversible. A wrong command can destroy a pool.</li>
<li><strong>R-D6-09-05:</strong> The institution must maintain at least one complete backup on ext4-formatted media at all times, ensuring data survivability even if ZFS becomes unavailable.</li>
<li><strong>R-D6-09-06:</strong> ZFS kernel module compatibility must be verified before any kernel update is applied to production systems. Kernel updates that break ZFS are deferred until a compatible ZFS version is available.</li>
<li><strong>R-D6-09-07:</strong> Pool capacity must never exceed 80% utilization. ZFS performance degrades significantly above this threshold, and the copy-on-write mechanism requires free space to function correctly. Exceeding 80% triggers an immediate storage expansion or data triage per D6-001.</li>
</ul>

</section>
<section>
<h2>Failure Modes</h2>

<ul>
<li>
<p><strong>Silent data corruption on non-checksumming filesystem.</strong> If ext4 or another non-checksumming filesystem is used for primary storage (violating R-D6-09-01), bit rot will go undetected. Data will degrade over years and the corruption will be discovered only when the data is needed. By that time, backups may also contain the corrupted version, having faithfully replicated the damage. Mitigation: R-D6-09-01 prohibits this configuration. D6-007 provides additional integrity verification layers.</p>
</li>
<li>
<p><strong>ZFS pool loss due to operator error.</strong> ZFS pool operations are powerful and irrevocable. A mistyped <code>zpool destroy</code> command eliminates an entire pool. A wrong <code>zpool create</code> command can overwrite existing data. Mitigation: R-D6-09-04 requires documentation before execution. The operator must type destructive commands manually, never from scripts, and must verify the target pool name before pressing enter. The <code>zpool destroy</code> command should be aliased to require confirmation.</p>
</li>
<li>
<p><strong>ZFS kernel module incompatibility.</strong> A kernel update breaks the ZFS module. The system boots but cannot mount ZFS pools. All data on ZFS is inaccessible. Mitigation: R-D6-09-06 requires compatibility verification before kernel updates. The institution maintains at least one previous kernel that is known to work with the installed ZFS version. The ext4 backup (R-D6-09-05) ensures data is accessible without ZFS.</p>
</li>
<li>
<p><strong>Pool capacity exhaustion.</strong> ZFS performance degrades severely when pools approach full capacity. Copy-on-write operations fail when there is no free space, potentially corrupting metadata. Mitigation: R-D6-09-07 sets an 80% threshold. Monitoring scripts alert when any pool exceeds 70%.</p>
</li>
<li>
<p><strong>Scrub neglect.</strong> Scrubs are deferred or forgotten. Corruption accumulates undetected. When finally discovered, the corruption may be widespread and unrecoverable. Mitigation: Scrubs are scheduled via systemd timers or cron, not dependent on operator memory. Missed scrubs generate alerts. Scrub completion is verified in the weekly operations review per OPS-001.</p>
</li>
<li>
<p><strong>Snapshot bloat.</strong> Snapshots accumulate without cleanup. Each snapshot holds references to changed blocks, preventing space reclamation. The pool fills up with snapshot overhead. Mitigation: The automated snapshot schedule in Section 4.6 includes retention limits. A weekly script verifies that snapshot cleanup is functioning and that snapshot space consumption is within expected bounds.</p>
</li>
</ul>

</section>
<section>
<h2>Recovery Procedures</h2>

<ol>
<li>
<p><strong>If ZFS module fails to load after kernel update.</strong> Boot to the previous known-good kernel (per D5-002, always maintain at least two bootable kernels). Mount ZFS pools. Do not apply the new kernel to other systems. Research ZFS compatibility with the new kernel version. Either wait for a ZFS update that supports the new kernel or hold the kernel at the current version until one is available. Document the incident.</p>
</li>
<li>
<p><strong>If a ZFS scrub reports unrepairable errors.</strong> Identify the affected files using <code>zpool status -v</code>. Check whether the affected files exist in backup. If they do, restore from backup. If they do not, attempt to read the affected files and assess the damage -- some files may be partially usable. Replace the disk exhibiting errors. After replacement, resilver the pool and run another scrub to verify. Document all affected files and their recovery status.</p>
</li>
<li>
<p><strong>If a ZFS pool runs a degraded vdev (disk failure in mirror or RAIDZ).</strong> Replace the failed disk immediately. Initiate resilver. Do not defer this -- a degraded pool has reduced redundancy and a second failure during this period could result in data loss. During resilver, minimize I/O load on the pool. Monitor resilver progress. After resilver completes, run a full scrub. Document the failure, replacement, and verification.</p>
</li>
<li>
<p><strong>If ZFS must be abandoned entirely.</strong> This is the nuclear option, invoked only if ZFS becomes unmaintainable (community collapse, irreconcilable kernel incompatibility, or discovery of a fundamental design flaw). The migration procedure: First, verify that the ext4 backup per R-D6-09-05 is current and complete. Second, select the replacement filesystem using the criteria in Section 4.1. Third, create the new filesystem on new or reformatted media. Fourth, copy all data from ZFS (while it still works) or from the ext4 backup to the new filesystem. Fifth, verify every file on the new filesystem against known-good checksums from D6-007. Sixth, update all procedures in this document and its dependents. This is a Tier 1 institutional event per GOV-001.</p>
</li>
<li>
<p><strong>If pool capacity exceeds 80%.</strong> Immediately assess what can be removed. Begin with Tier 4 (transient) data per D6-001. If removal of transient data is insufficient, escalate to Tier 3 (reference) data review. If the pool cannot be brought below 80% through data removal, add storage capacity (new vdevs). Do not wait for the pool to reach 90% -- recovery becomes exponentially more difficult as free space shrinks.</p>
</li>
</ol>

</section>
<section>
<h2>Evolution Path</h2>

<ul>
<li>
<p><strong>Years 0-5:</strong> The ZFS configuration is established and proven. The primary learning is operational -- developing familiarity with scrub management, snapshot workflows, and pool monitoring. The operator builds a baseline understanding of normal scrub durations, normal error rates (which should be zero), and normal capacity growth.</p>
</li>
<li>
<p><strong>Years 5-15:</strong> Hardware generations change. Disks are replaced. The first significant pool migrations occur -- replacing older, smaller disks with newer, larger ones. ZFS handles this through mirror replacement (replace one side of a mirror, resilver, replace the other side). The first serious evaluation of whether ZFS remains the correct choice happens at the ten-year mark.</p>
</li>
<li>
<p><strong>Years 15-30:</strong> The filesystem landscape may have changed dramatically. New filesystems may have emerged that surpass ZFS in integrity, performance, or community health. Btrfs may have matured to the point of reliability. Entirely new technologies (persistent memory filesystems, for example) may exist. The institution must evaluate these without sentimentality -- loyalty is to the data, not to the filesystem.</p>
</li>
<li>
<p><strong>Years 30-50+:</strong> The filesystem may have migrated one or more times. The critical requirement is that the evaluation criteria in Section 4.1 remain applicable regardless of which filesystem is current. New operators must understand why the filesystem was chosen, not just how to operate it.</p>
</li>
<li>
<p><strong>Signpost for revision:</strong> If ZFS kernel module compatibility requires intervention more than twice per year, or if the OpenZFS community shows signs of fragmentation or decline, begin evaluating alternatives immediately rather than waiting for the scheduled review.</p>
</li>
</ul>

</section>
<section>
<h2>Commentary Section</h2>

<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong>
The ZFS decision was not easy. The licensing situation genuinely concerns me. Every kernel update carries the risk that the ZFS module will not compile, and I will spend hours troubleshooting instead of doing productive work. I chose ZFS anyway because the alternative -- trusting unchecksummed data for fifty years -- is not really an alternative at all. Bit rot is real. I have seen it. Files that look fine until you open them and discover that the image is half-corrupted, the document is garbled, the database is silently missing records. ZFS catches these problems when they happen, not when it is too late.</p>
<p>The ext4 backup requirement is my safety net. If ZFS fails me catastrophically, I can still reach my data. It doubles the storage overhead for critical data. That is a price I am willing to pay for the ability to sleep at night.</p>
<p>I deliberately did not recommend Btrfs, and I suspect this will age poorly. Btrfs is improving rapidly and its kernel integration solves the most annoying problem with ZFS. If you are reading this in 2035 and Btrfs has had a decade of rock-solid operation, you should seriously consider migrating. Just do it methodically, with the ext4 safety net in place throughout the migration.</p>
<p>One warning about ZFS that the documentation does not emphasize enough: ZFS is opinionated about hardware. It wants ECC RAM. It performs best with dedicated disks, not partitions. It expects to manage the disks directly, not through a hardware RAID controller. Respect these opinions. ZFS was designed by people who thought carefully about data integrity, and fighting their design choices will cost you data.</p>

</section>
<section>
<h2>References</h2>

<ul>
<li>ETH-001 -- Ethical Foundations (Principle 2: Integrity Over Convenience; Principle 4: Longevity Over Novelty)</li>
<li>CON-001 -- The Founding Mandate (air-gap requirement, sovereignty)</li>
<li>SEC-001 -- Threat Model and Security Philosophy (Category 2: Data Integrity Threats)</li>
<li>OPS-001 -- Operations Philosophy (maintenance tempo, documentation-first principle)</li>
<li>D6-001 -- Data Philosophy (data tier system, preservation requirements, triage framework)</li>
<li>D6-005 -- Storage Architecture: Physical Media Strategy (hardware constraints on filesystem choice)</li>
<li>D6-006 -- Backup Doctrine (backup procedures that interact with filesystem snapshots)</li>
<li>D6-007 -- Data Integrity &amp; Verification (integrity verification layered above filesystem checksumming)</li>
<li>D5-002 -- Operating System Maintenance Procedures (kernel update procedures, ZFS module compatibility)</li>
<li>GOV-001 -- Authority Model (Tier 1 classification for filesystem migration decisions)</li>
<li>OpenZFS Project Documentation (archived locally per D6-014)</li>
<li>"ZFS on Linux" community knowledge base (archived locally)</li>
<li>Bonwick, Jeff and Moore, Bill. "ZFS: The Last Word in Filesystems." Sun Microsystems, 2005. (archived)</li>
</ul>
<hr />
<hr />
</section>
</article>
</main>
</body>
</html>