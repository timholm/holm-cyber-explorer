<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>STAGE 4: SPECIALIZED SYSTEMS -- RESEARCH &amp; THEORY ADVANCED - holm.chat</title>
<link rel="stylesheet" href="style.css">
<link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%2032%2032%22%3E%20%20%3Crect%20width=%2232%22%20height=%2232%22%20rx=%224%22%20fill=%22%231a1a2e%22/%3E%20%20%3Ctext%20x=%2216%22%20y=%2222%22%20font-family=%22monospace%22%20font-size=%2218%22%20font-weight=%22bold%22%20fill=%22%23e0e0e0%22%20text-anchor=%22middle%22%3EH%3C/text%3E%3C/svg%3E">
</head>
<body>
<header class="site-header">
<div class="header-inner">
<a href="index.html" class="site-title">holm.chat</a>
<span class="site-subtitle">Documentation Institution</span>
</div>
<nav class="site-nav"><ul>
<li><a href="index.html">Home</a></li>
<li><a href="domains-1-5.html">Domains 1-5</a></li>
<li><a href="domains-6-10.html">Domains 6-10</a></li>
<li><a href="domains-11-15.html">Domains 11-15</a></li>
<li><a href="domains-16-20.html">Domains 16-20</a></li>
<li><a href="meta-framework.html">Meta-Framework</a></li>
<li><a href="core-charter.html">Core Charter</a></li>
<li><a href="philosophy-batch2.html">Philosophy Batch2</a></li>
<li><a href="philosophy-batch3.html">Philosophy Batch3</a></li>
<li><a href="philosophy-batch4.html">Philosophy Batch4</a></li>
<li><a href="automation-ops.html">Automation Ops</a></li>
<li><a href="data-ops.html">Data Ops</a></li>
<li><a href="education-ops.html">Education Ops</a></li>
<li><a href="ethics-quality-ops.html">Ethics Quality Ops</a></li>
<li><a href="federation-ops.html">Federation Ops</a></li>
<li><a href="governance-ops.html">Governance Ops</a></li>
<li><a href="intel-ops.html">Intel Ops</a></li>
<li><a href="interface-ops.html">Interface Ops</a></li>
<li><a href="ops-batch1.html">Ops Batch1</a></li>
<li><a href="ops-batch2.html">Ops Batch2</a></li>
<li><a href="ops-batch3.html">Ops Batch3</a></li>
<li><a href="platform-ops.html">Platform Ops</a></li>
<li><a href="automation-advanced.html">Automation Advanced</a></li>
<li><a href="data-advanced.html">Data Advanced</a></li>
<li><a href="evolution-memory-advanced.html">Evolution Memory Advanced</a></li>
<li><a href="federation-import-advanced.html">Federation Import Advanced</a></li>
<li><a href="hic-architecture.html">Hic Architecture</a></li>
<li><a href="hic-interaction.html">Hic Interaction</a></li>
<li><a href="hic-knowledge-mapping.html">Hic Knowledge Mapping</a></li>
<li><a href="hic-master-blueprint.html">Hic Master Blueprint</a></li>
<li><a href="hic-offline-rendering.html">Hic Offline Rendering</a></li>
<li><a href="hic-spatial-data.html">Hic Spatial Data</a></li>
<li><a href="hic-visual-design.html">Hic Visual Design</a></li>
<li><a href="infrastructure-advanced.html">Infrastructure Advanced</a></li>
<li class="active"><a href="research-advanced.html">Research Advanced</a></li>
<li><a href="security-advanced.html">Security Advanced</a></li>
<li><a href="meta-batch1.html">Meta Batch1</a></li>
<li><a href="meta-batch2.html">Meta Batch2</a></li>
<li><a href="meta-batch3.html">Meta Batch3</a></li>
</ul></nav>
</header>
<div class="layout">
<aside class="sidebar">
<nav class="toc"><h2 class="toc-title">Table of Contents</h2><ul>
<li><a href="#stage-4-specialized-systems-research-theory-advanced">STAGE 4: SPECIALIZED SYSTEMS -- RESEARCH &amp; THEORY ADVANCED</a></li>
<ul>
<li><a href="#domain-14-articles-d14-002-through-d14-006">Domain 14 Articles D14-002 through D14-006</a></li>
<li><a href="#how-to-read-this-document">How to Read This Document</a></li>
</ul>
<li><a href="#d14-002-experimental-design-for-solo-researchers">D14-002 -- Experimental Design for Solo Researchers</a></li>
<ul>
<li><a href="#1-purpose">1. Purpose</a></li>
<li><a href="#2-scope">2. Scope</a></li>
<li><a href="#3-background">3. Background</a></li>
<ul>
<li><a href="#31-why-solo-experimentation-requires-its-own-methodology">3.1 Why Solo Experimentation Requires Its Own Methodology</a></li>
<li><a href="#32-the-pre-registration-principle">3.2 The Pre-Registration Principle</a></li>
<li><a href="#33-statistical-validity-and-honesty-about-sample-size">3.3 Statistical Validity and Honesty About Sample Size</a></li>
</ul>
<li><a href="#4-system-model">4. System Model</a></li>
<ul>
<li><a href="#41-the-experiment-lifecycle">4.1 The Experiment Lifecycle</a></li>
<li><a href="#42-variable-control-in-a-solo-environment">4.2 Variable Control in a Solo Environment</a></li>
<li><a href="#43-the-experiment-log-format">4.3 The Experiment Log Format</a></li>
</ul>
<li><a href="#5-rules-constraints">5. Rules &amp; Constraints</a></li>
<li><a href="#6-failure-modes">6. Failure Modes</a></li>
<li><a href="#7-recovery-procedures">7. Recovery Procedures</a></li>
<li><a href="#8-evolution-path">8. Evolution Path</a></li>
<li><a href="#9-commentary-section">9. Commentary Section</a></li>
<li><a href="#10-references">10. References</a></li>
</ul>
<li><a href="#d14-003-technology-evaluation-framework">D14-003 -- Technology Evaluation Framework</a></li>
<ul>
<li><a href="#1-purpose-1">1. Purpose</a></li>
<li><a href="#2-scope-1">2. Scope</a></li>
<li><a href="#3-background-1">3. Background</a></li>
<ul>
<li><a href="#31-the-technology-adoption-trap">3.1 The Technology Adoption Trap</a></li>
<li><a href="#32-the-air-gap-filter">3.2 The Air-Gap Filter</a></li>
<li><a href="#33-the-longevity-question">3.3 The Longevity Question</a></li>
</ul>
<li><a href="#4-system-model-1">4. System Model</a></li>
<ul>
<li><a href="#41-the-evaluation-rubric">4.1 The Evaluation Rubric</a></li>
<li><a href="#42-the-evaluation-process">4.2 The Evaluation Process</a></li>
<li><a href="#43-the-technology-decision-record-tdr">4.3 The Technology Decision Record (TDR)</a></li>
</ul>
<li><a href="#5-rules-constraints-1">5. Rules &amp; Constraints</a></li>
<li><a href="#6-failure-modes-1">6. Failure Modes</a></li>
<li><a href="#7-recovery-procedures-1">7. Recovery Procedures</a></li>
<li><a href="#8-evolution-path-1">8. Evolution Path</a></li>
<li><a href="#9-commentary-section-1">9. Commentary Section</a></li>
<li><a href="#10-references-1">10. References</a></li>
</ul>
<li><a href="#d14-004-knowledge-classification-and-taxonomy">D14-004 -- Knowledge Classification and Taxonomy</a></li>
<ul>
<li><a href="#1-purpose-2">1. Purpose</a></li>
<li><a href="#2-scope-2">2. Scope</a></li>
<li><a href="#3-background-2">3. Background</a></li>
<ul>
<li><a href="#31-the-problem-of-institutional-amnesia">3.1 The Problem of Institutional Amnesia</a></li>
<li><a href="#32-the-confidence-problem">3.2 The Confidence Problem</a></li>
<li><a href="#33-the-taxonomy-as-living-system">3.3 The Taxonomy as Living System</a></li>
</ul>
<li><a href="#4-system-model-2">4. System Model</a></li>
<ul>
<li><a href="#41-knowledge-categories">4.1 Knowledge Categories</a></li>
<li><a href="#42-confidence-levels">4.2 Confidence Levels</a></li>
<li><a href="#43-relationship-types">4.3 Relationship Types</a></li>
<li><a href="#44-the-knowledge-registry">4.4 The Knowledge Registry</a></li>
<li><a href="#45-the-taxonomy-review-process">4.5 The Taxonomy Review Process</a></li>
</ul>
<li><a href="#5-rules-constraints-2">5. Rules &amp; Constraints</a></li>
<li><a href="#6-failure-modes-2">6. Failure Modes</a></li>
<li><a href="#7-recovery-procedures-2">7. Recovery Procedures</a></li>
<li><a href="#8-evolution-path-2">8. Evolution Path</a></li>
<li><a href="#9-commentary-section-2">9. Commentary Section</a></li>
<li><a href="#10-references-2">10. References</a></li>
</ul>
<li><a href="#d14-005-negative-results-registry">D14-005 -- Negative Results Registry</a></li>
<ul>
<li><a href="#1-purpose-3">1. Purpose</a></li>
<li><a href="#2-scope-3">2. Scope</a></li>
<li><a href="#3-background-3">3. Background</a></li>
<ul>
<li><a href="#31-the-cost-of-forgetting-failure">3.1 The Cost of Forgetting Failure</a></li>
<li><a href="#32-the-stigma-of-negative-results">3.2 The Stigma of Negative Results</a></li>
<li><a href="#33-the-predecessor-problem">3.3 The Predecessor Problem</a></li>
</ul>
<li><a href="#4-system-model-3">4. System Model</a></li>
<ul>
<li><a href="#41-types-of-negative-results">4.1 Types of Negative Results</a></li>
<li><a href="#42-the-negative-results-entry-format">4.2 The Negative Results Entry Format</a></li>
<li><a href="#43-the-registry-search-protocol">4.3 The Registry Search Protocol</a></li>
<li><a href="#44-registry-maintenance">4.4 Registry Maintenance</a></li>
</ul>
<li><a href="#5-rules-constraints-3">5. Rules &amp; Constraints</a></li>
<li><a href="#6-failure-modes-3">6. Failure Modes</a></li>
<li><a href="#7-recovery-procedures-3">7. Recovery Procedures</a></li>
<li><a href="#8-evolution-path-3">8. Evolution Path</a></li>
<li><a href="#9-commentary-section-3">9. Commentary Section</a></li>
<li><a href="#10-references-3">10. References</a></li>
</ul>
<li><a href="#d14-006-multi-generational-research-continuity">D14-006 -- Multi-Generational Research Continuity</a></li>
<ul>
<li><a href="#1-purpose-4">1. Purpose</a></li>
<li><a href="#2-scope-4">2. Scope</a></li>
<li><a href="#3-background-4">3. Background</a></li>
<ul>
<li><a href="#31-the-successors-dilemma">3.1 The Successor&#x27;s Dilemma</a></li>
<li><a href="#32-research-state-as-institutional-infrastructure">3.2 Research State as Institutional Infrastructure</a></li>
<li><a href="#33-the-living-research-state-document">3.3 The Living Research State Document</a></li>
</ul>
<li><a href="#4-system-model-4">4. System Model</a></li>
<ul>
<li><a href="#41-the-living-research-state-document-lrsd">4.1 The Living Research State Document (LRSD)</a></li>
<li><a href="#42-the-research-handoff-protocol">4.2 The Research Handoff Protocol</a></li>
<li><a href="#43-the-research-continuity-audit">4.3 The Research Continuity Audit</a></li>
<li><a href="#44-long-term-research-programs">4.4 Long-Term Research Programs</a></li>
</ul>
<li><a href="#5-rules-constraints-4">5. Rules &amp; Constraints</a></li>
<li><a href="#6-failure-modes-4">6. Failure Modes</a></li>
<li><a href="#7-recovery-procedures-4">7. Recovery Procedures</a></li>
<li><a href="#8-evolution-path-4">8. Evolution Path</a></li>
<li><a href="#9-commentary-section-4">9. Commentary Section</a></li>
<li><a href="#10-references-4">10. References</a></li>
</ul></ul>
</nav>
</aside>
<main class="content">
<h1 id="stage-4-specialized-systems-research-theory-advanced">STAGE 4: SPECIALIZED SYSTEMS -- RESEARCH & THEORY ADVANCED</h1>
<h2 id="domain-14-articles-d14-002-through-d14-006">Domain 14 Articles D14-002 through D14-006</h2>
<p><strong>Document ID:</strong> STAGE4-RESEARCH-ADVANCED <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Classification:</strong> Stage 4 -- Specialized Systems. Advanced reference documents for the institution's research and knowledge infrastructure. These articles extend the philosophy established in D14-001 (Research Philosophy) into operational systems for experimentation, technology evaluation, knowledge organization, negative result management, and multi-generational research continuity. <strong>Depends On:</strong> ETH-001, CON-001, GOV-001, OPS-001, D14-001, D13-001, D11-001 <strong>Depended Upon By:</strong> All subsequent Domain 14 articles. Referenced by Domain 13 (Evolution) for validated methodology. Referenced by Domain 15 (Ethics & Safeguards) for research integrity context.</p>
<hr/>
<h2 id="how-to-read-this-document">How to Read This Document</h2>
<p>This document contains five advanced reference articles for Domain 14: Research & Theory. They are Stage 4 documents -- specialized systems that build upon the philosophy of D14-001 and the operational foundations of Stage 3. Where Stage 2 established why the institution conducts research, and Stage 3 established the basic procedures for doing so, Stage 4 provides the detailed systems that make research rigorous, organized, and sustainable across decades.</p>
<p>These articles are interconnected. D14-002 (Experimental Design) produces the data that D14-004 (Knowledge Classification) organizes. D14-005 (Negative Results Registry) captures what D14-002 discovers does not work. D14-003 (Technology Evaluation) applies the experimental methods of D14-002 to a specific and recurring research question. D14-006 (Multi-Generational Research Continuity) ensures that all of it survives the transition from one operator to the next.</p>
<p>If you are reading these for the first time, read them in order. The articles assume familiarity with D14-001 and will not re-argue the philosophical positions established there. They assume you understand the air-gap constraint, the echo-chamber danger, and the knowledge lifecycle. If those concepts are unfamiliar, stop here and read D14-001 first.</p>
<p>If you are a successor operator reading these for the first time, D14-006 is written specifically for you. Start there if you need immediate orientation on the state of the institution's research programs.</p>
<hr/>
<hr/>
<h1 id="d14-002-experimental-design-for-solo-researchers">D14-002 -- Experimental Design for Solo Researchers</h1>
<p><strong>Document ID:</strong> D14-002 <strong>Domain:</strong> 14 -- Research & Theory <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Depends On:</strong> ETH-001, CON-001, D14-001, OPS-001, GOV-001 <strong>Depended Upon By:</strong> D14-003, D14-005, D14-006. All Domain 14 articles that involve structured investigation. Referenced by Domain 13 for experimental validation of evolution proposals.</p>
<hr/>
<h2 id="1-purpose">1. Purpose</h2>
<p>This article defines how to design and execute rigorous experiments when the institution consists of a single researcher operating in isolation. It addresses the fundamental challenge that most experimental methodology was developed for research teams -- groups of people who can divide the roles of experimenter, observer, analyst, and critic. When all of those roles collapse into one person, the standard methodology does not simply scale down. It breaks in specific, predictable ways, and those breakages must be addressed with deliberate structural countermeasures.</p>
<p>The purpose is not to pretend that solo research can achieve the same rigor as a well-funded laboratory with peer review. It cannot. D14-001 is honest about that limitation (per ETH-001, Principle 6). The purpose is to extract the maximum possible rigor from a single-person operation -- to identify which elements of experimental methodology still apply, which must be adapted, and which must be replaced with alternatives that serve the same epistemological function.</p>
<p>This article also defines the experiment log format -- the standardized documentation structure that ensures every experiment conducted by this institution is recorded completely enough to be understood, evaluated, and replicated by a future operator who was not present when it was conducted.</p>
<h2 id="2-scope">2. Scope</h2>
<p><strong>In scope:</strong></p>
<ul>
<li>Experimental design principles adapted for a single researcher with no external peer review.</li>
<li>Variable identification and control in small-scale, resource-constrained environments.</li>
<li>Blinding and bias mitigation strategies for solo operation.</li>
<li>Statistical validity at small sample sizes, including when statistics are and are not appropriate.</li>
<li>The experiment log format: mandatory fields, optional fields, completion criteria.</li>
<li>Pre-registration of hypotheses and success criteria before experimentation begins.</li>
<li>The relationship between experiment design and the knowledge classification system (D14-004).</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>The philosophical justification for research (D14-001).</li>
<li>Technology evaluation specifically (D14-003, though it uses these methods).</li>
<li>Knowledge taxonomy management (D14-004).</li>
<li>Negative results documentation format (D14-005, though experimental failures feed that registry).</li>
<li>Multi-generational continuity of research programs (D14-006).</li>
</ul>
<h2 id="3-background">3. Background</h2>
<h3 id="31-why-solo-experimentation-requires-its-own-methodology">3.1 Why Solo Experimentation Requires Its Own Methodology</h3>
<p>The standard scientific method assumes, implicitly, a social infrastructure. Hypotheses are tested by experimenters who report results to peers who scrutinize the methodology, attempt replication, and challenge the conclusions. The rigor of the method depends on this social process as much as it depends on the experimental procedure itself. Remove the social process, and the methodology must compensate.</p>
<p>A solo researcher faces three specific challenges that team-based research does not:</p>
<p>First, there is no role separation. The person who designs the experiment, the person who executes it, the person who observes the results, and the person who interprets them are all the same individual. This means the biases that role separation is designed to mitigate -- observer bias, confirmation bias, expectation effects -- are all fully active in every phase of every experiment.</p>
<p>Second, there is no independent replication. The results of any experiment are correlated with the experimenter's equipment, environment, habits, and blind spots. D14-001 describes adapted replication strategies (temporal, methodological, environmental, documentation-based), but these are mitigations, not solutions.</p>
<p>Third, there is no external calibration. In a research community, the researcher learns what constitutes sufficient evidence by observing the standards applied to others' work. In isolation, the researcher must set their own evidentiary standards, which means those standards are subject to the same biases they are meant to control.</p>
<h3 id="32-the-pre-registration-principle">3.2 The Pre-Registration Principle</h3>
<p>The single most important safeguard against confirmation bias in solo research is pre-registration: the practice of writing down the hypothesis, the experimental procedure, the variables, and the success/failure criteria before the experiment begins. Pre-registration does not prevent bias -- nothing fully prevents bias in a solo operation. But it makes bias visible. When the results arrive and the experimenter is tempted to reinterpret the success criteria to fit the data, the pre-registered criteria are there in writing, immutable, forcing a confrontation between what was expected and what was found.</p>
<p>Pre-registration is mandatory in this institution (see R-EXP-01). It is not a formality. It is the primary structural defense against the most common experimental failure mode: unconsciously adjusting the question to fit the answer.</p>
<h3 id="33-statistical-validity-and-honesty-about-sample-size">3.3 Statistical Validity and Honesty About Sample Size</h3>
<p>Many experiments conducted by this institution will involve small sample sizes -- often extremely small. When you are testing whether a particular backup strategy is reliable, you may run it ten times. When you are evaluating hardware durability, your sample may be a single unit. When you are comparing two approaches to a task, your sample is your own performance under two conditions.</p>
<p>Statistical methods designed for large samples do not apply here, and pretending they do is worse than not using statistics at all. A p-value calculated from three data points is not meaningful. A confidence interval from a sample of one is fiction.</p>
<p>This article takes the position that honest qualitative assessment, clearly labeled as such, is superior to dishonest quantitative analysis. When sample sizes support statistical analysis, use it. When they do not, describe what you observed, acknowledge the limitations, and assign an appropriate confidence level through the knowledge classification system. Do not dress opinion in the costume of data.</p>
<h2 id="4-system-model">4. System Model</h2>
<h3 id="41-the-experiment-lifecycle">4.1 The Experiment Lifecycle</h3>
<p>Every experiment in this institution follows a defined lifecycle with five phases:</p>
<p><strong>Phase 1: Question Formulation.</strong> The experiment begins with a question that is specific enough to be testable. "Is ZFS a good filesystem?" is not a testable question. "Does ZFS scrub detect and correct single-bit errors on the institution's hardware within 24 hours?" is testable. The question formulation phase ends with a written question and a clear statement of why the answer matters to the institution.</p>
<p><strong>Phase 2: Pre-Registration.</strong> Before any experimental work begins, the experimenter writes and files a pre-registration document containing: the hypothesis (what the experimenter expects to find), the null hypothesis (what the result looks like if the hypothesis is wrong), the experimental procedure, the variables (independent, dependent, and controlled), the sample size and justification, the success criteria, the failure criteria, and the planned analysis method. The pre-registration is timestamped and filed in the research archive. It cannot be modified after filing; if the experimenter realizes the design is flawed, a new pre-registration is filed and the old one is retained with a note explaining the revision.</p>
<p><strong>Phase 3: Execution.</strong> The experiment is conducted according to the pre-registered procedure. Deviations from the procedure are recorded in real time in the experiment log. The experimenter records raw observations, not interpretations. "The scrub completed in 4 hours 17 minutes and reported 0 errors" is a raw observation. "The scrub worked as expected" is an interpretation, and it belongs in Phase 4.</p>
<p><strong>Phase 4: Analysis.</strong> The raw data is analyzed according to the pre-registered analysis method. The analysis compares the observed results against the pre-registered success and failure criteria. The analysis must explicitly address: did the results match the hypothesis, the null hypothesis, or neither? Were there unexpected observations? Were there deviations from the procedure that may have affected the results? The analysis is documented in full, including the reasoning process, not just the conclusion.</p>
<p><strong>Phase 5: Classification and Archival.</strong> The results are classified according to D14-004 (Knowledge Classification) and archived according to the research archive procedures. If the results are negative (the hypothesis was disproven or the experiment was inconclusive), an entry is created in the Negative Results Registry (D14-005).</p>
<h3 id="42-variable-control-in-a-solo-environment">4.2 Variable Control in a Solo Environment</h3>
<p>Classical experimental design distinguishes between independent variables (what the experimenter manipulates), dependent variables (what the experimenter measures), and controlled variables (what the experimenter holds constant). In a solo environment with limited resources, controlling variables is the primary methodological challenge.</p>
<p><strong>Environmental controls.</strong> Many experiments are conducted on the institution's production or test hardware, which means the environment cannot be fully isolated. The experimenter must document the environmental conditions under which the experiment is conducted: what else was running on the system, what the ambient temperature was, what the power state was, what the time of day was. These become part of the experiment record and allow future analysis of whether environmental factors affected the results.</p>
<p><strong>Temporal controls.</strong> Experiments should be conducted at consistent times and under consistent conditions when possible. If an experiment must be run multiple times, the runs should be spaced to avoid temporal confounds (such as system load varying by time of day, or battery state varying by season).</p>
<p><strong>Blinding adaptations.</strong> True blinding -- where the experimenter does not know which condition they are testing -- is generally impossible in a solo environment. The adapted alternatives are: delayed analysis (record raw data, then analyze it at least 48 hours later, after the emotional investment in the outcome has faded), pre-registered criteria (the success/failure criteria are locked before the data arrives), and quantitative measurement (where possible, use numerical measurements rather than subjective assessments, as numbers are harder to unconsciously distort).</p>
<h3 id="43-the-experiment-log-format">4.3 The Experiment Log Format</h3>
<p>Every experiment is documented in a standardized experiment log with the following mandatory fields:</p>
<ul>
<li><strong>Experiment ID:</strong> A unique identifier in the format EXP-YYYY-NNN (year and sequential number).</li>
<li><strong>Date initiated:</strong> The date pre-registration was filed.</li>
<li><strong>Date completed:</strong> The date the final analysis was written.</li>
<li><strong>Question:</strong> The specific, testable question.</li>
<li><strong>Hypothesis:</strong> What the experimenter expected to find.</li>
<li><strong>Null hypothesis:</strong> What the result looks like if the hypothesis is wrong.</li>
<li><strong>Variables:</strong> Independent, dependent, and controlled.</li>
<li><strong>Procedure:</strong> Step-by-step instructions for executing the experiment.</li>
<li><strong>Sample size and justification:</strong> How many trials, and why that number.</li>
<li><strong>Success criteria:</strong> Pre-registered, specific, measurable.</li>
<li><strong>Failure criteria:</strong> Pre-registered, specific, measurable.</li>
<li><strong>Raw data:</strong> Observations recorded during execution, without interpretation.</li>
<li><strong>Deviations:</strong> Any departures from the pre-registered procedure.</li>
<li><strong>Analysis:</strong> Interpretation of the data against the criteria.</li>
<li><strong>Conclusion:</strong> The result, classified by confidence level per D14-004.</li>
<li><strong>Negative result entry:</strong> If applicable, reference to the D14-005 registry entry.</li>
<li><strong>Implications:</strong> What this result means for institutional practice.</li>
<li><strong>Related experiments:</strong> Cross-references to prior or subsequent experiments.</li>
<li><strong>Adversarial review:</strong> The documented attempt to disprove the conclusion (mandatory for verified results per D14-001, R-RES-04).</li>
</ul>
<p>Optional fields include: photographs or diagrams, equipment serial numbers, software versions, environmental measurements, and links to raw data files.</p>
<h2 id="5-rules-constraints">5. Rules & Constraints</h2>
<ul>
<li><strong>R-EXP-01:</strong> Pre-registration is mandatory for all formal experiments. No experimental result may be classified above "speculative" without a pre-registration document filed before execution began.</li>
<li><strong>R-EXP-02:</strong> The experiment log format (Section 4.3) is mandatory for all formal experiments. Informal observations and quick tests do not require the full format but must still be documented in the operational log with a note indicating they were not conducted under formal experimental protocols.</li>
<li><strong>R-EXP-03:</strong> Raw data must be recorded during execution, not reconstructed afterward. If raw data was not recorded, the experiment's maximum confidence classification is "provisional," regardless of the result.</li>
<li><strong>R-EXP-04:</strong> No experiment may be conducted on production systems without a documented risk assessment approved through the appropriate governance tier per GOV-001 and D14-001 R-RES-08. Test environments must be used unless the question specifically requires production conditions, and this requirement must be documented and justified.</li>
<li><strong>R-EXP-05:</strong> Statistical methods may only be applied when the sample size and data characteristics justify their use. The experimenter must document why a given statistical test is appropriate. If the sample size does not support statistical analysis, the result must be reported as a qualitative observation with an explicit statement of the sample limitation.</li>
<li><strong>R-EXP-06:</strong> Every experiment must be documented completely enough that a future operator, unfamiliar with the experiment, could reproduce the procedure from the documentation alone. This is tested by the documentation replication method described in D14-001, Section 4.3.</li>
<li><strong>R-EXP-07:</strong> Experiments that produce negative results must file an entry in the Negative Results Registry (D14-005) within 30 days of experiment completion.</li>
</ul>
<h2 id="6-failure-modes">6. Failure Modes</h2>
<ul>
<li><strong>Pre-registration drift.</strong> The experimenter modifies the pre-registration after seeing preliminary results, consciously or unconsciously adjusting the criteria to fit the data. Mitigation: pre-registration documents are timestamped and filed immutably. Revisions require a new filing with an explanation. The original is never deleted.</li>
<li><strong>Observer effect contamination.</strong> The experimenter's expectations influence what they observe, particularly with subjective measurements. Mitigation: prefer quantitative measurement. Use delayed analysis. Pre-register success criteria in numerical terms where possible.</li>
<li><strong>Sample size delusion.</strong> The experimenter treats a small number of observations as statistically significant, applying formal tests that are not valid for the sample size. Mitigation: R-EXP-05 requires explicit justification of statistical methods. The default is qualitative assessment with acknowledged limitations.</li>
<li><strong>Procedure drift.</strong> Over the course of a multi-session experiment, the experimenter gradually deviates from the pre-registered procedure without noticing or recording the deviations. Mitigation: the experiment log requires real-time recording of deviations. The procedure should be physically present (printed or displayed) during each experimental session.</li>
<li><strong>Analysis paralysis.</strong> The experimenter generates so much raw data that analysis becomes overwhelming and is deferred indefinitely. Mitigation: pre-register the analysis method. The analysis method defines what data is relevant to the conclusion. Everything else is supplementary.</li>
<li><strong>Rigor theater.</strong> The experimenter follows the formal procedures -- pre-registration, log format, analysis -- without genuine intellectual engagement. The forms are completed, but the adversarial self-critique required by D14-001 is absent. Mitigation: the adversarial review field in the experiment log is not optional for verified results. The review must contain a genuine attempt to disprove the conclusion, and "I could not think of any objections" is not a genuine attempt.</li>
</ul>
<h2 id="7-recovery-procedures">7. Recovery Procedures</h2>
<ol>
<li><strong>If pre-registration has been neglected:</strong> Do not retroactively create pre-registrations for completed experiments. That defeats their purpose. Instead, classify all un-pre-registered results as "provisional" at best. Begin pre-registering all new experiments immediately. Review the un-pre-registered results with explicit awareness that confirmation bias may have affected the conclusions.</li>
<li><strong>If experiment logs are incomplete:</strong> Reconstruct what can be reconstructed from operational logs, notes, and memory. Mark reconstructed entries explicitly. Accept that incompletely documented experiments carry lower confidence. Establish the habit of recording in real time by keeping the log physically present during experimentation.</li>
<li><strong>If statistical methods have been misapplied:</strong> Conduct a statistical audit. For each experiment that used formal statistical analysis, verify that the sample size and data characteristics justified the method used. Re-analyze with appropriate methods or downgrade to qualitative assessment. Update the knowledge classification accordingly.</li>
<li><strong>If rigor theater is occurring:</strong> This is the hardest failure to recover from because it requires honest self-assessment. Review the last five adversarial reviews. For each, ask: did I genuinely try to disprove this conclusion? If the adversarial reviews are pro forma, re-conduct them with a specific mandate: identify at least one plausible alternative explanation or methodological weakness for each result. If none can be found, that is acceptable -- but the search must be genuine.</li>
<li><strong>If experiments are being avoided entirely:</strong> Return to D14-001 R-RES-06, which requires at least two formal investigations per year. Identify the simplest, lowest-risk experimental question relevant to current institutional operations. Design and execute it. The goal is to restart the practice, not to produce significant results.</li>
</ol>
<h2 id="8-evolution-path">8. Evolution Path</h2>
<ul>
<li><strong>Years 0-5:</strong> Experimental methodology is being learned and practiced. The first experiments will likely be crude -- simple comparisons, basic performance measurements, straightforward technology tests. This is expected. The value of these early experiments is in building the habit and refining the process, not in producing groundbreaking results. The experiment log format may need adjustment as practical experience reveals fields that are missing or unnecessary.</li>
<li><strong>Years 5-15:</strong> The experiment archive grows. Patterns emerge across experiments. The experimenter becomes better at identifying confounds, designing controls, and recognizing their own biases. Temporal replication of early experiments becomes possible and may reveal that some early conclusions were wrong. This is a feature, not a failure.</li>
<li><strong>Years 15-30:</strong> If succession has occurred, the new operator brings genuinely different biases and perspectives to experimental design. This is the closest the institution comes to independent replication. The experiment archive serves as a training resource, showing the successor how the institution conducts research and what has been learned.</li>
<li><strong>Years 30-50+:</strong> The experiment archive is a longitudinal dataset spanning decades. It contains not just individual results but the institution's evolving understanding of its own systems. Hardware generations, software paradigms, and operational practices have changed. The archive records how those changes were evaluated and adopted.</li>
</ul>
<h2 id="9-commentary-section">9. Commentary Section</h2>
<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong> I am writing an experimental methodology document for a laboratory that consists of one person, one desk, and whatever hardware I have built. The absurdity is not lost on me. But the absurdity is the point. The experiments I conduct here -- on backup reliability, on hardware longevity, on software stability -- are not academic exercises. They are the basis for decisions that affect whether this institution survives the next decade. If I test a backup strategy badly and conclude it works when it does not, I will discover the error at the worst possible time: during a recovery. So I will test carefully, document thoroughly, and be honest about what the results do and do not tell me. The formality is not for show. It is for the version of me who needs to trust these results at 3 AM during an actual failure.</p>
<h2 id="10-references">10. References</h2>
<ul>
<li>ETH-001 -- Ethical Foundations (Principle 2: Integrity Over Convenience; Principle 6: Honest Accounting of Limitations)</li>
<li>CON-001 -- The Founding Mandate (self-contained knowledge, lifetime operation)</li>
<li>GOV-001 -- Authority Model (governance tiers for experiments on production systems)</li>
<li>OPS-001 -- Operations Philosophy (documentation-first, operational logging)</li>
<li>D14-001 -- Research Philosophy (knowledge lifecycle, anti-echo-chamber structures, replication adaptations, R-RES-04, R-RES-06, R-RES-08)</li>
<li>D14-004 -- Knowledge Classification and Taxonomy (confidence levels, classification system)</li>
<li>D14-005 -- Negative Results Registry (documentation of failed experiments)</li>
<li>Stage 1 Documentation Framework, Domain 14 (article list, dependencies)</li>
</ul>
<hr/>
<hr/>
<h1 id="d14-003-technology-evaluation-framework">D14-003 -- Technology Evaluation Framework</h1>
<p><strong>Document ID:</strong> D14-003 <strong>Domain:</strong> 14 -- Research & Theory <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Depends On:</strong> ETH-001, CON-001, GOV-001, SEC-001, OPS-001, D14-001, D14-002, D13-001 <strong>Depended Upon By:</strong> D14-004, D14-005, D14-006. All Domain 13 (Evolution) articles involving technology adoption. All domain articles involving technology selection.</p>
<hr/>
<h2 id="1-purpose-1">1. Purpose</h2>
<p>This article defines the framework by which this institution evaluates new technologies for potential adoption. It is one of the most consequential research activities the institution conducts, because technology adoption decisions are among the hardest to reverse. A filesystem choice may last decades. A database engine becomes the foundation of years of accumulated data. A hardware platform determines what software can run and what upgrades are possible. These decisions must be made carefully, with a structured process that resists both the excitement of novelty and the inertia of familiarity.</p>
<p>D14-001 establishes that the institution must avoid both stagnation and reckless adoption. This article operationalizes that balance by providing a repeatable evaluation framework -- a rubric, a process, and a documentation format that ensures every technology decision is made with full awareness of its implications and recorded thoroughly enough that future operators understand why the decision was made.</p>
<p>The framework is designed for the specific constraints of an air-gapped, off-grid, single-operator institution. The evaluation criteria differ substantially from those used by internet-connected organizations. Uptime SLAs, cloud integration, automatic update mechanisms, and vendor support contracts are irrelevant here. What matters is longevity, maintainability without internet access, air-gap compatibility, documentation quality, and the health of the community that produces the technology -- because that community is the source of future updates, knowledge, and fixes that must be manually transported across the air gap.</p>
<h2 id="2-scope-1">2. Scope</h2>
<p><strong>In scope:</strong></p>
<ul>
<li>The technology evaluation rubric: criteria, weighting, and scoring.</li>
<li>The evaluation process: from initial identification through proof-of-concept to adoption decision.</li>
<li>Air-gap compatibility assessment: specific tests and requirements.</li>
<li>Documentation quality assessment: what constitutes sufficient documentation for an isolated institution.</li>
<li>Community health assessment: indicators of project longevity and sustainability.</li>
<li>The proof-of-concept procedure: how to test a technology safely before committing to it.</li>
<li>The Technology Decision Record (TDR): format and filing requirements.</li>
<li>The relationship between technology evaluation and the evolution framework (D13-001).</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>General experimental methodology (D14-002, though technology evaluation uses those methods).</li>
<li>Specific technology choices for specific domains (individual domain articles).</li>
<li>The procurement process for acquiring technology across the air gap (D11-001, ADM-series).</li>
<li>Security assessment of new software (SEC-001, SEC-series, though security is one evaluation criterion).</li>
</ul>
<h2 id="3-background-1">3. Background</h2>
<h3 id="31-the-technology-adoption-trap">3.1 The Technology Adoption Trap</h3>
<p>Technology adoption in an air-gapped institution presents a double bind. On one side is the danger of stagnation: clinging to known technology until it becomes unsupportable, its community disbands, and its documentation rots. On the other side is the danger of churn: adopting every promising new technology, enduring constant migration, and never achieving the stability that a fifty-year institution requires.</p>
<p>Both dangers are real. Both have destroyed institutions. The history of computing is littered with organizations that bet on technologies that seemed permanent at the time -- and equally littered with organizations that refused to move until it was too late to move gracefully.</p>
<p>The evaluation framework exists to navigate between these dangers. It provides a structured basis for the question: should we adopt this technology? And it provides an equally structured basis for the question: should we stay with what we have?</p>
<h3 id="32-the-air-gap-filter">3.2 The Air-Gap Filter</h3>
<p>The air gap is the most powerful filter in the technology evaluation framework. Technologies that require constant internet connectivity are immediately disqualified. Technologies that depend on remote repositories for routine operation are disqualified. Technologies whose update mechanism assumes network access are not disqualified but receive a significant penalty in the evaluation rubric, because every update will require manual intervention to transport across the air gap.</p>
<p>This filter eliminates a large fraction of modern software. That elimination is a feature, not a bug. The remaining candidates -- software that can be installed from local media, operated without network access, updated through manual package installation, and documented thoroughly enough to troubleshoot without searching the internet -- are inherently more suitable for this institution's constraints.</p>
<h3 id="33-the-longevity-question">3.3 The Longevity Question</h3>
<p>The most important evaluation criterion, and the hardest to assess, is longevity. Will this technology still be viable in ten years? Twenty? Will its community still exist? Will its file formats still be readable? Will replacement hardware still run it?</p>
<p>No one can predict the future with certainty. But there are indicators. Technologies with open-source licenses are more likely to survive than proprietary ones, because the code persists even if the original developer abandons it. Technologies with simple, well-documented file formats are more resilient than those with opaque binary formats. Technologies with large, diverse communities are more likely to survive the loss of any single contributor than those dependent on a single developer.</p>
<p>The evaluation rubric encodes these indicators. It does not predict the future. It assesses the probability of survival based on observable present characteristics.</p>
<h2 id="4-system-model-1">4. System Model</h2>
<h3 id="41-the-evaluation-rubric">4.1 The Evaluation Rubric</h3>
<p>Every technology evaluation uses a standardized rubric with six criteria, each scored on a five-point scale (1 = unacceptable, 2 = poor, 3 = adequate, 4 = good, 5 = excellent):</p>
<p><strong>Criterion 1: Longevity (weight 3x).</strong> How likely is this technology to remain viable over 10-20+ years? Indicators include: age of the project, size and diversity of the contributor community, adoption breadth, license type (open source strongly preferred), governance structure, historical stability, and the existence of a clear succession plan or foundation stewardship. A technology that has been stable for a decade scores higher than one released last year, regardless of feature comparison.</p>
<p><strong>Criterion 2: Maintainability (weight 3x).</strong> How difficult is it to maintain this technology without internet access and without vendor support? Indicators include: complexity of configuration, clarity of error messages, quality of logging, dependency count and depth, build reproducibility, and the skill level required for routine maintenance. A technology that the operator can fully understand and troubleshoot is more valuable than one that is technically superior but opaque.</p>
<p><strong>Criterion 3: Air-Gap Compatibility (weight 3x).</strong> Can this technology be installed, operated, updated, and maintained entirely offline? Specific tests include: installation from local media, operation without any network call, update application from locally-stored packages, license validation without phone-home, and documentation availability offline. Any technology that phones home for license validation receives an automatic score of 1.</p>
<p><strong>Criterion 4: Documentation Quality (weight 2x).</strong> How good is the available documentation, and can it be stored locally? Indicators include: completeness, accuracy, currency, availability in offline-friendly formats (HTML dump, PDF, man pages, plain text), quality of error documentation, availability of architecture documents (not just tutorials), and the presence of troubleshooting guides for common failure modes.</p>
<p><strong>Criterion 5: Data Sovereignty (weight 2x).</strong> Does the institution retain full control of its data when using this technology? Indicators include: open and documented file/storage formats, availability of export tools, absence of vendor lock-in mechanisms, ability to read data without the original software, and compatibility with the institution's backup and archive strategies.</p>
<p><strong>Criterion 6: Community Health (weight 1x).</strong> What is the current state of the project's community? Indicators include: contributor activity (commits, releases, bug fixes), responsiveness to bug reports, quality of community discourse, diversity of contributors (not dependent on a single person or company), and the trajectory (growing, stable, or declining).</p>
<p>The weighted score produces a total out of 70 (maximum). The following thresholds guide -- but do not dictate -- the adoption decision:</p>
<ul>
<li><strong>56-70 (Strong Candidate):</strong> Technology meets or exceeds institutional requirements. Proceed to proof-of-concept.</li>
<li><strong>42-55 (Qualified Candidate):</strong> Technology has notable strengths but also significant gaps. Proceed to proof-of-concept only if the gaps can be mitigated and the alternative is worse.</li>
<li><strong>28-41 (Weak Candidate):</strong> Technology has fundamental concerns. Do not proceed unless no alternative exists and the need is critical.</li>
<li><strong>Below 28 (Disqualified):</strong> Technology is unsuitable for this institution.</li>
</ul>
<h3 id="42-the-evaluation-process">4.2 The Evaluation Process</h3>
<p>Technology evaluation follows a defined sequence:</p>
<p><strong>Step 1: Identification.</strong> A technology is identified as a potential candidate, either during a scheduled technology review cycle or in response to a specific institutional need. The identification includes a brief statement of what problem the technology would solve and why current solutions are insufficient.</p>
<p><strong>Step 2: Preliminary Assessment.</strong> The technology is evaluated against the rubric using publicly available information (documentation, community forums captured during air-gap crossings, published reviews from the institution's reference library). This is a desk evaluation, not a hands-on test. The goal is to determine whether the technology merits the investment of a proof-of-concept.</p>
<p><strong>Step 3: Proof-of-Concept.</strong> If the preliminary assessment scores 42 or above, a proof-of-concept is conducted. The proof-of-concept is a structured experiment following D14-002 methodology: pre-registered hypothesis, defined success criteria, controlled execution, documented results. The proof-of-concept tests the technology on non-production systems under conditions that approximate production use. Minimum duration: 30 days of active use, or the duration necessary to exercise all major features relevant to the institution's use case, whichever is longer.</p>
<p><strong>Step 4: Decision.</strong> The proof-of-concept results, the rubric score, and the overall assessment are compiled into a Technology Decision Record (TDR). The adoption decision is made through the appropriate governance tier per GOV-001. The TDR is filed in the research archive regardless of the decision.</p>
<p><strong>Step 5: Integration or Rejection.</strong> If adopted, the technology is integrated through the evolution framework (D13-001). If rejected, the TDR is filed in the research archive and referenced in the Negative Results Registry (D14-005) if the rejection was based on specific technical failures discovered during the proof-of-concept.</p>
<h3 id="43-the-technology-decision-record-tdr">4.3 The Technology Decision Record (TDR)</h3>
<p>Every technology evaluation produces a TDR with the following mandatory fields:</p>
<ul>
<li><strong>TDR ID:</strong> Format TDR-YYYY-NNN.</li>
<li><strong>Date:</strong> Date of final decision.</li>
<li><strong>Technology evaluated:</strong> Name, version, source.</li>
<li><strong>Problem statement:</strong> What institutional need prompted the evaluation.</li>
<li><strong>Current solution:</strong> What the institution currently uses for this need (if anything).</li>
<li><strong>Rubric scores:</strong> All six criteria with individual scores, justifications, and weighted total.</li>
<li><strong>Proof-of-concept summary:</strong> Duration, scope, key findings (or "Not conducted" with justification).</li>
<li><strong>Decision:</strong> Adopt, Reject, or Defer (with conditions for revisiting).</li>
<li><strong>Rationale:</strong> The reasoning behind the decision, including dissenting considerations.</li>
<li><strong>Migration plan reference:</strong> If adopted, reference to the migration plan in Domain 13.</li>
<li><strong>Review date:</strong> When this decision should be revisited (mandatory, even for rejections).</li>
<li><strong>Related TDRs:</strong> Cross-references to evaluations of competing or complementary technologies.</li>
</ul>
<h2 id="5-rules-constraints-1">5. Rules & Constraints</h2>
<ul>
<li><strong>R-TEV-01:</strong> No technology may be adopted for production use without a completed Technology Decision Record. Emergency adoptions (e.g., during disaster recovery) must have a TDR filed within 90 days of adoption.</li>
<li><strong>R-TEV-02:</strong> The evaluation rubric (Section 4.1) must be applied in full for every formal evaluation. Individual criteria may not be skipped because the evaluator considers them irrelevant. If a criterion is genuinely not applicable, it must be scored and the justification for the score documented.</li>
<li><strong>R-TEV-03:</strong> Proof-of-concept testing must be conducted on non-production systems unless the nature of the evaluation specifically requires production conditions, and this requirement is documented and approved per GOV-001.</li>
<li><strong>R-TEV-04:</strong> Technologies with an Air-Gap Compatibility score of 1 may not be adopted under any circumstances. The air gap is non-negotiable per CON-001 and SEC-001.</li>
<li><strong>R-TEV-05:</strong> All Technology Decision Records must be retained permanently in the research archive, including records for rejected technologies. Rejection records are as valuable as adoption records -- they prevent future operators from re-evaluating technologies that have already been found unsuitable.</li>
<li><strong>R-TEV-06:</strong> Every adopted technology must have a scheduled review date, no more than five years from adoption, at which the technology is re-evaluated against the current rubric. Technologies may be re-confirmed, scheduled for replacement, or flagged for active migration.</li>
<li><strong>R-TEV-07:</strong> Technology evaluations must include an exit assessment: how difficult would it be to migrate away from this technology if it proves unsuitable after adoption? Technologies with high exit costs must score correspondingly higher on all other criteria to justify the risk.</li>
</ul>
<h2 id="6-failure-modes-1">6. Failure Modes</h2>
<ul>
<li><strong>Novelty bias.</strong> The evaluator is excited by a new technology and unconsciously inflates rubric scores to justify adoption. Mitigation: the rubric requires written justification for each score. The adversarial review from D14-001 applies to technology evaluations. A second evaluation of the same technology after a 30-day cooling period is recommended for high-impact adoptions.</li>
<li><strong>Familiarity bias.</strong> The evaluator unconsciously penalizes new technologies to justify staying with current solutions. Mitigation: the rubric must also be applied to the current solution. If the current solution scores lower than the candidate, the burden of justification shifts to explaining why the current solution should be retained.</li>
<li><strong>Proof-of-concept theater.</strong> The proof-of-concept is conducted as a formality, using unrealistically favorable conditions that do not reflect actual institutional use. Mitigation: the proof-of-concept must follow D14-002 experimental methodology with pre-registered success criteria. The conditions must approximate production use, including failure injection where appropriate.</li>
<li><strong>Decision record neglect.</strong> Technology decisions are made informally without completing the TDR. Over time, no one remembers why a particular technology was chosen. Mitigation: R-TEV-01 makes the TDR mandatory. No technology is considered officially adopted without one.</li>
<li><strong>Review date amnesia.</strong> The scheduled review date passes without a review. Technologies that should have been replaced or re-evaluated persist on inertia. Mitigation: review dates are recorded in the institutional scheduling system per D11-001. They are treated as mandatory calendar events.</li>
<li><strong>Community collapse blindness.</strong> An adopted technology's community deteriorates, but the institution does not notice because it is behind the air gap. Mitigation: the air-gap crossing process (Domain 18) should include a scheduled check of adopted technology communities. Community health is a persistent monitoring obligation, not a one-time assessment.</li>
</ul>
<h2 id="7-recovery-procedures-1">7. Recovery Procedures</h2>
<ol>
<li><strong>If technologies have been adopted without TDRs:</strong> Conduct retroactive evaluations. Apply the rubric to all currently adopted technologies. File TDRs for each. Identify any technologies that would not have passed the evaluation and schedule replacement planning for them.</li>
<li><strong>If review dates have been missed:</strong> Conduct the reviews immediately. Prioritize technologies that have been in use longest without review. Accept that some reviews will reveal that the institution has been using unsuitable technologies, and begin migration planning where necessary.</li>
<li><strong>If a community collapse has been discovered:</strong> Assess the severity. If the technology is still functional but unsupported, begin searching for alternatives immediately. If the technology is actively deteriorating (bugs not fixed, security vulnerabilities unpatched), escalate to a priority evaluation and migration per the evolution framework.</li>
<li><strong>If the evaluation process has degraded into formality:</strong> Re-read this article and D14-001. Conduct a genuine, adversarial evaluation of the most recently adopted technology. If the evaluation reveals concerns that were missed, re-evaluate other recent adoptions with the same rigor.</li>
<li><strong>If exit costs are discovered to be higher than assessed:</strong> Document the true exit costs. Update the TDR. If the technology must still be replaced, the exit cost assessment informs the migration timeline. Use this experience to improve exit cost assessment in future evaluations.</li>
</ol>
<h2 id="8-evolution-path-1">8. Evolution Path</h2>
<ul>
<li><strong>Years 0-5:</strong> The framework is being applied to the institution's founding technology stack. These initial evaluations establish the baseline TDR library and calibrate the rubric against real-world experience. Some rubric weights may need adjustment as the institution discovers which criteria are most predictive of actual suitability.</li>
<li><strong>Years 5-15:</strong> The TDR library grows into a decision history. Patterns become visible: which criteria best predicted success? Which technologies lasted? Which failed unexpectedly? These patterns should inform rubric refinements. Scheduled reviews of adopted technologies begin producing re-evaluation data.</li>
<li><strong>Years 15-30:</strong> Technology generations have turned over. The institution has experienced at least one major technology migration and can evaluate the framework's effectiveness based on how well it predicted success and failure. The framework itself is a technology that must be evaluated and evolved.</li>
<li><strong>Years 30-50+:</strong> The TDR library is a multi-decade record of the institution's technology choices. It tells the story of what was tried, what worked, what failed, and why. For a successor operator, it is an invaluable guide to the institution's technological identity and the reasoning behind it.</li>
</ul>
<h2 id="9-commentary-section-1">9. Commentary Section</h2>
<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong> Every technology decision I make now is a bet on the future. Some of those bets will be wrong. The purpose of the TDR is not to guarantee that every bet wins -- that is impossible. The purpose is to ensure that every bet is made consciously, with documented reasoning, so that the person who has to live with the consequences (whether that is future-me or a successor) knows why the bet was placed and can decide intelligently whether to let it ride or cut losses. The worst technology decision is not the one that turns out to be wrong. It is the one that nobody remembers making, for reasons nobody recorded, that nobody knows how to reverse.</p>
<h2 id="10-references-1">10. References</h2>
<ul>
<li>ETH-001 -- Ethical Foundations (Principle 2: Integrity Over Convenience; Principle 6: Honest Accounting)</li>
<li>CON-001 -- The Founding Mandate (air-gap constraint, lifetime operation, self-sovereignty)</li>
<li>GOV-001 -- Authority Model (decision tiers for technology adoption)</li>
<li>SEC-001 -- Threat Model and Security Philosophy (air-gap requirements, supply chain risk)</li>
<li>OPS-001 -- Operations Philosophy (sustainability, complexity budget)</li>
<li>D14-001 -- Research Philosophy (knowledge lifecycle, anti-echo-chamber structures)</li>
<li>D14-002 -- Experimental Design for Solo Researchers (proof-of-concept methodology)</li>
<li>D13-001 -- Evolution Philosophy (technology migration, change management)</li>
<li>D11-001 -- Administration Philosophy (resource allocation, scheduling)</li>
<li>D14-005 -- Negative Results Registry (documentation of rejected technologies)</li>
</ul>
<hr/>
<hr/>
<h1 id="d14-004-knowledge-classification-and-taxonomy">D14-004 -- Knowledge Classification and Taxonomy</h1>
<p><strong>Document ID:</strong> D14-004 <strong>Domain:</strong> 14 -- Research & Theory <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Depends On:</strong> ETH-001, CON-001, GOV-001, OPS-001, D14-001, D14-002 <strong>Depended Upon By:</strong> D14-005, D14-006. All Domain 14 articles that produce or consume classified knowledge. All domains that reference institutional knowledge with confidence levels.</p>
<hr/>
<h2 id="1-purpose-2">1. Purpose</h2>
<p>This article defines how the institution organizes what it knows. Knowledge without organization is a pile. Knowledge with organization is a library. The difference between the two is not the knowledge itself but the structures that make it findable, assessable, and usable. This article provides those structures.</p>
<p>D14-001 introduced the knowledge lifecycle (observation, investigation, validation, classification, integration, archival) and established four confidence levels (verified, provisional, speculative, deprecated). This article expands those concepts into a complete taxonomy -- a system of categories, relationships, and confidence levels that allows the institution to answer three critical questions about any piece of knowledge: What kind of knowledge is this? How confident are we in it? How does it relate to other things we know?</p>
<p>These questions matter because knowledge that cannot be found is knowledge that does not exist, operationally speaking. Knowledge whose confidence level is unknown is either trusted too much or too little. Knowledge whose relationships are untracked creates blind spots -- the institution changes one thing without realizing it depends on another. The taxonomy is the defense against all three failures.</p>
<h2 id="2-scope-2">2. Scope</h2>
<p><strong>In scope:</strong></p>
<ul>
<li>The knowledge taxonomy: categories, subcategories, and classification rules.</li>
<li>Confidence levels: definitions, assignment criteria, promotion and demotion procedures.</li>
<li>Relationship types: dependencies, derivations, contradictions, and supersessions.</li>
<li>The classification process: how new knowledge is classified upon creation.</li>
<li>The taxonomy review process: how the taxonomy itself evolves.</li>
<li>Cross-domain knowledge integration: how knowledge from one domain relates to knowledge in others.</li>
<li>The knowledge registry: the master index of all classified institutional knowledge.</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>The philosophical justification for research and knowledge management (D14-001).</li>
<li>Experimental methodology for producing new knowledge (D14-002).</li>
<li>Technology evaluation specifically (D14-003).</li>
<li>Negative results documentation format (D14-005, though negative results are classified within this taxonomy).</li>
<li>Physical storage and preservation of knowledge artifacts (RES-013, archival procedures).</li>
</ul>
<h2 id="3-background-2">3. Background</h2>
<h3 id="31-the-problem-of-institutional-amnesia">3.1 The Problem of Institutional Amnesia</h3>
<p>Institutions forget. Not through dramatic data loss -- though that happens -- but through the slow accumulation of knowledge that is recorded but not organized, filed but not findable, known to one person but not externalized into the institutional record. When the person who knows something leaves, retires, or simply forgets, the knowledge vanishes. The institution then rediscovers it, repeating work that was already done, or worse, proceeds without it and makes decisions based on incomplete understanding.</p>
<p>This institution is especially vulnerable to institutional amnesia because it is operated by one person. There is no colleague to remind you that "we tried that in 2029 and it did not work." There is no institutional folklore passed through conversation. If knowledge is not in the record, it is not in the institution.</p>
<p>The taxonomy is the antidote. By requiring that all significant knowledge be classified, indexed, and related to other knowledge, it creates a structure that survives individual memory failure.</p>
<h3 id="32-the-confidence-problem">3.2 The Confidence Problem</h3>
<p>Not all knowledge is equally reliable, and treating it as such is dangerous. A result that has been replicated through multiple methods over five years is not the same as a preliminary observation from a single experiment. A procedure that has been tested in disaster recovery drills is not the same as one that was written but never executed. Yet without a formal confidence system, both occupy the same status in the institutional record: they are written down, and they are assumed to be true.</p>
<p>The confidence level system introduced in D14-001 and formalized here addresses this problem. Every piece of classified knowledge carries an explicit confidence level that tells the reader: this is how much you should trust this. The levels are not grades. "Speculative" is not a failing mark -- it is an honest assessment that the evidence is preliminary. "Verified" is not a gold star -- it is a statement that specific evidentiary criteria have been met. The system values honesty over prestige.</p>
<h3 id="33-the-taxonomy-as-living-system">3.3 The Taxonomy as Living System</h3>
<p>A taxonomy created at founding and never revised will become useless within a decade. The institution's knowledge will outgrow the categories. New domains of inquiry will emerge that do not fit the original structure. Relationships between knowledge items will become apparent that the original taxonomy did not anticipate.</p>
<p>The taxonomy must therefore be a living system -- regularly reviewed, periodically revised, and designed from the start to accommodate growth. This article defines not just the taxonomy but the process for evolving it.</p>
<h2 id="4-system-model-2">4. System Model</h2>
<h3 id="41-knowledge-categories">4.1 Knowledge Categories</h3>
<p>The taxonomy organizes institutional knowledge into seven primary categories:</p>
<p><strong>Category 1: Technical Knowledge.</strong> How things work. Hardware specifications, software behavior, system configurations, performance characteristics, failure modes, and troubleshooting procedures. This is the institution's understanding of its own infrastructure.</p>
<p><strong>Category 2: Procedural Knowledge.</strong> How things are done. Step-by-step procedures, workflows, checklists, and protocols. This includes everything from daily maintenance routines to disaster recovery sequences. Procedural knowledge is distinct from technical knowledge: you can understand how a filesystem works (technical) without knowing the institution's specific procedure for checking filesystem integrity (procedural).</p>
<p><strong>Category 3: Analytical Knowledge.</strong> What has been learned through investigation. Experimental results, comparative analyses, technology evaluations, performance studies, and failure analyses. This is the output of the research function defined in D14-001 and executed through D14-002.</p>
<p><strong>Category 4: Contextual Knowledge.</strong> Why things are the way they are. Design decisions, architectural rationales, historical context, and the reasoning behind institutional choices. This is the knowledge that answers "why?" rather than "how?" -- and it is often the first knowledge lost in institutional transitions, because people document what they did but not why they did it.</p>
<p><strong>Category 5: Environmental Knowledge.</strong> What exists outside the institution. Technology trends, community health assessments, supply chain information, regulatory changes, and external threat intelligence. This knowledge is gathered during air-gap crossings and from the institution's reference library. It is inherently less current than internal knowledge and must be dated and contextualized accordingly.</p>
<p><strong>Category 6: Negative Knowledge.</strong> What does not work, what has been disproven, and what has been tried and abandoned. This category is the complement of the Negative Results Registry (D14-005). It is perhaps the most valuable category for preventing repeated failures, and it is the category most institutions neglect because recording failures feels unproductive.</p>
<p><strong>Category 7: Meta-Knowledge.</strong> Knowledge about the institution's knowledge. The taxonomy itself, the classification procedures, the confidence level definitions, the research methodology, and the evolution processes. Meta-knowledge is what allows the institution to manage and improve its own knowledge infrastructure.</p>
<h3 id="42-confidence-levels">4.2 Confidence Levels</h3>
<p>Each classified knowledge item carries one of four confidence levels, as introduced in D14-001:</p>
<p><strong>Verified.</strong> The knowledge has been established through rigorous investigation (D14-002), has survived adversarial review (D14-001, Section 4.2), and has been replicated through at least one adapted replication method (temporal, methodological, environmental, or documentation-based). Verified knowledge may be used as a basis for operational decisions and cited as evidence in further research. Promotion to Verified requires: a completed experiment log with pre-registration, a documented adversarial review, and a documented replication.</p>
<p><strong>Provisional.</strong> The knowledge is supported by a single investigation or a limited evidence base. It has not been replicated. It may be used cautiously in operational decisions with explicit acknowledgment of its provisional status. Most new experimental results enter the taxonomy at this level. Promotion to Provisional requires: a completed experiment log or a thorough analytical assessment with documented methodology and evidence.</p>
<p><strong>Speculative.</strong> The knowledge is based on preliminary observation, theoretical reasoning, or incomplete evidence. It should not be used as a basis for significant operational decisions without further investigation. Speculative knowledge is valuable because it identifies areas for future research. Entry at Speculative requires: a documented observation or hypothesis with a clear statement of the evidence supporting it and the evidence lacking.</p>
<p><strong>Deprecated.</strong> The knowledge was previously classified at a higher level but has been contradicted by subsequent evidence, superseded by better knowledge, or invalidated by changed conditions. Deprecated knowledge is not deleted from the registry. It is retained with its deprecation justification and a reference to the knowledge that supersedes it. Deprecated knowledge serves as a historical record and a warning against re-adoption.</p>
<h3 id="43-relationship-types">4.3 Relationship Types</h3>
<p>Knowledge items in the taxonomy are connected through four relationship types:</p>
<p><strong>Depends-on.</strong> Knowledge item A depends on knowledge item B if A's validity requires B's validity. If B is deprecated, A must be reviewed and potentially deprecated or re-validated independently.</p>
<p><strong>Derived-from.</strong> Knowledge item A is derived from knowledge item B if A was produced through investigation or analysis of B. The derivation chain is a provenance record that allows future operators to trace conclusions back to their evidence base.</p>
<p><strong>Contradicts.</strong> Knowledge item A contradicts knowledge item B if they cannot both be true. Contradictions must be resolved through further investigation or by deprecating one item. Unresolved contradictions are flagged in the registry and prioritized for research.</p>
<p><strong>Supersedes.</strong> Knowledge item A supersedes knowledge item B if A is a more recent, more complete, or more accurate treatment of the same subject. Supersession does not delete B -- it retains B as historical context and marks A as the current authoritative source.</p>
<h3 id="44-the-knowledge-registry">4.4 The Knowledge Registry</h3>
<p>The knowledge registry is the master index of all classified institutional knowledge. It is a structured document (or set of documents) in plain text format, compliant with the 50-year continuity rules, containing for each entry:</p>
<ul>
<li><strong>Knowledge ID:</strong> Format KR-YYYY-NNN (year and sequential number).</li>
<li><strong>Title:</strong> A concise description of the knowledge item.</li>
<li><strong>Category:</strong> One of the seven primary categories (Section 4.1).</li>
<li><strong>Confidence level:</strong> Verified, Provisional, Speculative, or Deprecated.</li>
<li><strong>Date classified:</strong> When the item was added to the registry.</li>
<li><strong>Date last reviewed:</strong> When the item's classification was last confirmed or changed.</li>
<li><strong>Source:</strong> Reference to the experiment log, analysis, observation, or external source.</li>
<li><strong>Relationships:</strong> Depends-on, Derived-from, Contradicts, and Supersedes links.</li>
<li><strong>Summary:</strong> A brief (one to three paragraph) statement of the knowledge.</li>
<li><strong>Full reference:</strong> Pointer to the complete document, experiment log, or data source.</li>
</ul>
<h3 id="45-the-taxonomy-review-process">4.5 The Taxonomy Review Process</h3>
<p>The taxonomy itself is reviewed on a defined cycle:</p>
<p><strong>Annual classification audit.</strong> Once per year, a random sample of at least 10% of registry entries is reviewed for accuracy of classification. Are the confidence levels still appropriate? Are the relationships still current? Has any knowledge been superseded without the registry being updated? The audit results are documented and any corrections are applied immediately.</p>
<p><strong>Triennial taxonomy review.</strong> Every three years, the category structure itself is reviewed. Are the seven categories still adequate? Do any categories need to be split, merged, or redefined? Are there significant bodies of knowledge that do not fit any existing category? The review may result in taxonomy modifications, which are documented as taxonomy evolution events in the registry.</p>
<p><strong>Event-triggered review.</strong> Certain events trigger an immediate taxonomy review of affected entries: a major technology migration, a disaster recovery event, a succession event, or the deprecation of a significant body of knowledge. The triggered review covers all knowledge items related to the event and ensures the registry accurately reflects the institution's post-event knowledge state.</p>
<h2 id="5-rules-constraints-2">5. Rules & Constraints</h2>
<ul>
<li><strong>R-KCT-01:</strong> Every significant knowledge item produced by the institution's research function must be classified and entered in the knowledge registry within 60 days of its production. "Significant" means any knowledge that may inform an operational decision or be referenced by a future investigation.</li>
<li><strong>R-KCT-02:</strong> Confidence levels must be assigned based on the criteria defined in Section 4.2, not on the operator's intuitive sense of how reliable the knowledge is. The criteria are the standard, not the feeling.</li>
<li><strong>R-KCT-03:</strong> Relationships between knowledge items must be documented at the time of classification. The classifier must actively check for dependencies, derivations, contradictions, and supersessions with existing registry entries.</li>
<li><strong>R-KCT-04:</strong> Deprecated knowledge must never be deleted from the registry. Deprecation is a status change, not a deletion. The full history of a knowledge item -- including its original classification, any promotions or demotions, and its eventual deprecation -- must be preserved.</li>
<li><strong>R-KCT-05:</strong> The annual classification audit (Section 4.5) is mandatory and must be completed within the first quarter of each calendar year. The results must be documented in the research archive.</li>
<li><strong>R-KCT-06:</strong> Taxonomy modifications (changes to the category structure, confidence level definitions, or relationship types) are Tier 2 decisions under GOV-001 and require the 30-day waiting period. The taxonomy is institutional infrastructure and must not be changed casually.</li>
<li><strong>R-KCT-07:</strong> All knowledge registry data must be stored in plain text formats compliant with the 50-year continuity rules. The registry must be readable without any specialized software.</li>
</ul>
<h2 id="6-failure-modes-2">6. Failure Modes</h2>
<ul>
<li><strong>Classification backlog.</strong> Knowledge is produced faster than it is classified. The registry falls behind, and an increasing body of institutional knowledge exists outside the taxonomy. Mitigation: R-KCT-01's 60-day deadline. If the backlog grows despite this, reduce the classification effort per item (a brief classification is better than no classification) or increase the time allocated to classification in the operational tempo.</li>
<li><strong>Confidence inflation.</strong> Knowledge is routinely assigned higher confidence than the evidence supports, typically because the operator wants to feel certain rather than provisional. Mitigation: R-KCT-02 requires criteria-based assignment. The annual audit (R-KCT-05) provides a systematic check.</li>
<li><strong>Relationship neglect.</strong> Knowledge items are classified individually but their relationships are not documented. The registry becomes a flat list rather than a connected graph. Mitigation: R-KCT-03 requires active relationship checking at classification time. The audit verifies relationship documentation.</li>
<li><strong>Taxonomy ossification.</strong> The taxonomy is never revised, and knowledge items are forced into categories that no longer fit. Mitigation: the triennial taxonomy review (Section 4.5) is mandatory. The review explicitly asks whether the categories are still adequate.</li>
<li><strong>Registry as bureaucracy.</strong> The classification process becomes so burdensome that the operator avoids producing new knowledge to avoid the classification work. Mitigation: the classification process should be lightweight -- a structured entry in a plain text file, not an elaborate production. If classification takes more than 30 minutes per item on average, the process should be simplified.</li>
<li><strong>Deprecated knowledge zombies.</strong> Knowledge that has been deprecated is still being used as the basis for operational decisions because operators check the procedure but not the registry. Mitigation: procedures that reference knowledge items should include the knowledge ID. When a knowledge item is deprecated, a search of all documents referencing it should be conducted and references updated.</li>
</ul>
<h2 id="7-recovery-procedures-2">7. Recovery Procedures</h2>
<ol>
<li><strong>If a classification backlog has accumulated:</strong> Conduct a classification sprint. Classify all backlogged items at the minimum viable level: category, confidence level, one-sentence summary, and source reference. Relationships can be documented in a second pass. The priority is getting everything into the registry, even if the entries are sparse.</li>
<li><strong>If confidence levels have been inflated:</strong> Conduct a confidence audit. Review all "verified" items. For each, check: is there a documented replication? Is there a documented adversarial review? If not, downgrade to "provisional." Review all "provisional" items. For each, check: is there a completed experiment log? If not, downgrade to "speculative."</li>
<li><strong>If relationships have been neglected:</strong> Conduct a relationship review. For each registry entry, check: does this item depend on other items? Does it derive from evidence? Does it contradict anything? Does it supersede an older entry? Document all identified relationships. This is labor-intensive but is a one-time recovery that, once completed, is maintained incrementally.</li>
<li><strong>If the taxonomy has ossified:</strong> Conduct an emergency taxonomy review. Identify all knowledge items that do not fit their assigned categories. Look for patterns -- do the misfits suggest a new category is needed? Revise the taxonomy and reclassify affected items. File the revision as a Tier 2 decision per R-KCT-06.</li>
<li><strong>If the registry has been abandoned entirely:</strong> Begin rebuilding from the research archive. Every experiment log, every TDR, every negative result entry is a knowledge item that should be in the registry. Classify them systematically, starting with the most recent and working backward. Accept that the early reconstruction will be incomplete and refine it over time.</li>
</ol>
<h2 id="8-evolution-path-2">8. Evolution Path</h2>
<ul>
<li><strong>Years 0-5:</strong> The taxonomy is new and sparsely populated. The seven categories and four confidence levels should be sufficient for the institution's initial knowledge base. The operator is learning to classify consistently. Some miscategorization is inevitable and will be corrected in early audits.</li>
<li><strong>Years 5-15:</strong> The registry grows to hundreds of entries. Relationships between knowledge items become increasingly important. The registry begins to function as a genuine knowledge graph -- a map of what the institution knows and how its knowledge connects. The triennial taxonomy review may reveal that one or two categories need subdivision.</li>
<li><strong>Years 15-30:</strong> Succession is likely during this period. The taxonomy is one of the most valuable resources the successor inherits, because it provides an organized map of everything the institution has learned. The taxonomy review following succession is critically important -- the successor may organize knowledge differently than the founder, and the taxonomy should accommodate that.</li>
<li><strong>Years 30-50+:</strong> The registry contains the institution's entire intellectual history. Deprecated items show what was once believed and later disproven. Relationship chains show how understanding evolved. The registry is not just an index -- it is a narrative of institutional learning.</li>
</ul>
<h2 id="9-commentary-section-2">9. Commentary Section</h2>
<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong> I resist taxonomy. My natural instinct is to throw knowledge into a wiki and rely on search. But search finds what you are looking for. Taxonomy reveals what you did not know to look for. The relationship links, especially, are the value proposition: when I deprecate a piece of knowledge, I need to know what else depends on it. When I discover a contradiction, I need to know which items are affected. A flat file system cannot provide this. A taxonomy can. So I will build the taxonomy, I will maintain it, and I will resist the temptation to let it atrophy when it feels like overhead. It is not overhead. It is the institution's nervous system.</p>
<h2 id="10-references-2">10. References</h2>
<ul>
<li>ETH-001 -- Ethical Foundations (Principle 6: Honest Accounting of Limitations)</li>
<li>CON-001 -- The Founding Mandate (knowledge preservation, 50-year continuity)</li>
<li>GOV-001 -- Authority Model (Tier 2 decisions for taxonomy changes)</li>
<li>OPS-001 -- Operations Philosophy (documentation-first, operational tempo)</li>
<li>D14-001 -- Research Philosophy (knowledge lifecycle, confidence levels, anti-echo-chamber structures)</li>
<li>D14-002 -- Experimental Design (experiment log format, pre-registration)</li>
<li>D14-005 -- Negative Results Registry (Category 6: Negative Knowledge)</li>
<li>D14-006 -- Multi-Generational Research Continuity (taxonomy as succession resource)</li>
<li>META-00-ART-001 -- Stage 1 Meta-Framework (50-year continuity rules, plain text requirements)</li>
</ul>
<hr/>
<hr/>
<h1 id="d14-005-negative-results-registry">D14-005 -- Negative Results Registry</h1>
<p><strong>Document ID:</strong> D14-005 <strong>Domain:</strong> 14 -- Research & Theory <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Depends On:</strong> ETH-001, CON-001, D14-001, D14-002, D14-004 <strong>Depended Upon By:</strong> D14-003, D14-006. All Domain 14 articles that initiate new investigations. Referenced by all domains where prior failures are relevant to current work.</p>
<hr/>
<h2 id="1-purpose-3">1. Purpose</h2>
<p>This article defines the Negative Results Registry -- the institution's formal record of what has been tried and failed, what has been hypothesized and disproven, and what has been evaluated and rejected. It establishes why this registry exists, how it is structured, how entries are created, and how the registry is used to prevent the institution from repeating known failures.</p>
<p>The concept is simple and the resistance to it is powerful. Human beings do not naturally document failure. Failure feels like wasted effort, and documenting it feels like memorializing the waste. The instinct is to move on, to try the next thing, to focus on what works rather than what does not. This instinct, in a single-operator institution with no external peer community, is catastrophic over time. It guarantees that failures are repeated, that rejected approaches are revisited without memory of the prior rejection, and that the institution spends its finite research resources rediscovering things it already knows do not work.</p>
<p>D14-001 established the principle (R-RES-02) that negative results must be documented with the same rigor as positive results. This article operationalizes that principle into a specific system.</p>
<h2 id="2-scope-3">2. Scope</h2>
<p><strong>In scope:</strong></p>
<ul>
<li>The philosophical and practical justification for maintaining a negative results registry.</li>
<li>The negative results entry format: mandatory fields, optional fields, completion criteria.</li>
<li>Types of negative results: experimental failures, disproven hypotheses, rejected technologies, abandoned approaches.</li>
<li>The registry search protocol: how to consult the registry before starting new work.</li>
<li>Integration with the knowledge taxonomy (D14-004) and the experiment log format (D14-002).</li>
<li>The registry maintenance process: review, consolidation, and cross-referencing.</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>General experimental methodology (D14-002).</li>
<li>General knowledge classification (D14-004).</li>
<li>Technology evaluation procedures (D14-003, though rejected technology evaluations feed this registry).</li>
<li>Operational incident reports (Domain 10, though operational failures may generate negative result entries).</li>
</ul>
<h2 id="3-background-3">3. Background</h2>
<h3 id="31-the-cost-of-forgetting-failure">3.1 The Cost of Forgetting Failure</h3>
<p>In the broader scientific community, the problem of unreported negative results -- sometimes called the "file drawer problem" -- is well recognized. Journals preferentially publish positive results. Researchers preferentially submit positive results. The result is a systematic distortion of the knowledge base: the published record suggests that interventions work more often than they actually do, because the failures are sitting in file drawers, unreported.</p>
<p>In a single-operator institution, the file drawer is the operator's memory, and memory is the least reliable file drawer of all. The operator runs an experiment. It fails. The operator moves on. Three years later, the operator -- or worse, a successor -- faces the same question, runs the same experiment, and discovers the same failure. The time, energy, and materials spent on the second attempt are pure waste, waste that a one-line entry in a registry would have prevented.</p>
<p>The cost of forgetting failure compounds. Each unreported negative result is a trap waiting for a future researcher. In an institution that operates for decades, the cumulative waste of repeated failures can dwarf the cost of maintaining the registry by orders of magnitude.</p>
<h3 id="32-the-stigma-of-negative-results">3.2 The Stigma of Negative Results</h3>
<p>The most powerful obstacle to maintaining a negative results registry is not logistical -- it is psychological. Recording a failure feels like admitting defeat. The experiment log for a failed experiment is, the instinct whispers, a monument to wasted effort. This instinct must be confronted directly.</p>
<p>A failed experiment is not wasted effort. It is knowledge. It tells you that a particular approach, under particular conditions, does not produce the expected result. That knowledge has concrete value: it narrows the search space for future work, it prevents resource expenditure on known dead ends, and it sometimes reveals unexpected insights about why something does not work that illuminate how other things do work.</p>
<p>Furthermore, per ETH-001 Principle 6, the institution must honestly account for its limitations. A research archive that contains only successes is not honest. It presents a distorted picture of the institution's knowledge by omitting the attempts that did not succeed. The negative results registry is, in this sense, an ethical obligation, not just a practical tool.</p>
<h3 id="33-the-predecessor-problem">3.3 The Predecessor Problem</h3>
<p>Perhaps the most important justification for the negative results registry is the predecessor problem: the situation where a successor operator, unfamiliar with the institution's research history, embarks on a line of investigation that their predecessor already pursued and abandoned. Without a negative results registry, the successor has no way to know this. The predecessor's failures exist only in the predecessor's memory, which is no longer available.</p>
<p>The registry is, in this context, a letter from the past to the future: "I tried this. Here is what happened. Here is why I stopped. If you want to try it again, at least you know where I got stuck."</p>
<h2 id="4-system-model-3">4. System Model</h2>
<h3 id="41-types-of-negative-results">4.1 Types of Negative Results</h3>
<p>The registry accommodates four types of negative results:</p>
<p><strong>Type 1: Experimental Failures.</strong> An experiment was conducted per D14-002, and the hypothesis was disproven. The experiment produced a clear negative result. This is the cleanest type of negative result and the easiest to document, because the experiment log already contains most of the necessary information.</p>
<p><strong>Type 2: Inconclusive Investigations.</strong> An investigation was conducted but produced no clear result -- the evidence was insufficient, the methodology was flawed, or external factors prevented completion. Inconclusive results are negative results in the sense that they did not achieve their objective, even if they did not disprove the hypothesis. They are recorded because they contain valuable information about methodology limitations and environmental constraints.</p>
<p><strong>Type 3: Technology Rejections.</strong> A technology was evaluated per D14-003 and rejected. The Technology Decision Record (TDR) documents the evaluation. The registry entry cross-references the TDR and captures the specific reasons for rejection in a form that is quickly searchable.</p>
<p><strong>Type 4: Abandoned Approaches.</strong> A line of inquiry or an operational approach was pursued for a period, found to be unproductive or impractical, and abandoned. This type is the most subjective and the hardest to document because the abandonment often happened gradually rather than at a clear decision point. It is also the type most prone to omission, and therefore the type where the registry adds the most value.</p>
<h3 id="42-the-negative-results-entry-format">4.2 The Negative Results Entry Format</h3>
<p>Each registry entry contains the following mandatory fields:</p>
<ul>
<li><strong>NR ID:</strong> Format NR-YYYY-NNN (year and sequential number).</li>
<li><strong>Date:</strong> Date the entry was filed.</li>
<li><strong>Type:</strong> One of the four types defined in Section 4.1.</li>
<li><strong>Title:</strong> A concise, descriptive title. The title should be findable: a future researcher scanning the registry for relevant prior work should be able to identify relevance from the title alone.</li>
<li><strong>Summary:</strong> A one-paragraph summary of what was attempted, what was expected, and what actually happened.</li>
<li><strong>Methodology reference:</strong> Reference to the experiment log (D14-002), TDR (D14-003), or other documentation of the investigation.</li>
<li><strong>Root cause analysis:</strong> Why did this fail? What was the mechanism of failure? This is the most valuable field in the entry because it allows future researchers to determine whether the failure was inherent (the approach is fundamentally flawed) or conditional (the approach might work under different conditions).</li>
<li><strong>Conditions and constraints:</strong> Under what specific conditions was this attempted? Hardware, software versions, environmental conditions, and any other contextual factors. A failure under specific conditions does not necessarily mean failure under all conditions.</li>
<li><strong>Residual value:</strong> Did the failed investigation produce any useful knowledge, even though it did not achieve its primary objective? Unexpected observations, methodology insights, and incidental discoveries are documented here.</li>
<li><strong>Recommendations for future work:</strong> Should this approach be revisited? Under what changed conditions might it succeed? Or is the failure fundamental enough that the approach should be considered permanently closed? This field is the predecessor's advice to the future.</li>
<li><strong>Knowledge registry cross-reference:</strong> The D14-004 Knowledge ID for the corresponding negative knowledge entry.</li>
<li><strong>Related entries:</strong> Cross-references to other negative results entries on similar topics.</li>
</ul>
<h3 id="43-the-registry-search-protocol">4.3 The Registry Search Protocol</h3>
<p>Before beginning any new formal investigation (D14-002) or technology evaluation (D14-003), the researcher must search the negative results registry for relevant prior work. This is a mandatory step, not a suggested one. The search protocol is:</p>
<p><strong>Step 1: Keyword search.</strong> Search the registry titles and summaries for terms related to the planned investigation.</p>
<p><strong>Step 2: Category search.</strong> Search by knowledge category (D14-004) for entries in the same domain as the planned investigation.</p>
<p><strong>Step 3: Relationship search.</strong> Check the knowledge registry (D14-004) for knowledge items related to the planned investigation and follow any links to negative results entries.</p>
<p><strong>Step 4: Document the search.</strong> Record the search in the pre-registration document (D14-002, Section 4.1, Phase 2). List all relevant negative results entries found and explain how the planned investigation differs from or builds upon the prior work. If no relevant entries are found, record that the search was conducted and found nothing.</p>
<p>The search documentation serves two purposes: it ensures the search was actually done (not just claimed), and it provides the researcher with relevant context before the investigation begins.</p>
<h3 id="44-registry-maintenance">4.4 Registry Maintenance</h3>
<p>The registry requires periodic maintenance to remain useful:</p>
<p><strong>Annual consolidation.</strong> Once per year, review the registry for entries that can be consolidated. If five separate experiments have all failed to achieve the same objective for the same reason, a single consolidated entry may be clearer than five individual ones. The individual entries are retained but linked to the consolidation.</p>
<p><strong>Cross-reference verification.</strong> During the annual review, verify that all cross-references (to experiment logs, TDRs, knowledge registry entries, and other negative results entries) are still valid. Update broken references.</p>
<p><strong>Relevance review.</strong> Some negative results become irrelevant over time -- the technology they evaluated no longer exists, the hardware they tested has been retired, the conditions they operated under no longer apply. These entries are not deleted (per D14-004, R-KCT-04, nothing is deleted) but may be marked as "historically relevant only" to help current researchers focus on entries that are still operationally pertinent.</p>
<h2 id="5-rules-constraints-3">5. Rules & Constraints</h2>
<ul>
<li><strong>R-NRR-01:</strong> Every formal experiment (D14-002) that fails to confirm its hypothesis must have a negative results entry filed within 30 days of the experiment's completion. This is a restatement and operationalization of D14-001, R-RES-02.</li>
<li><strong>R-NRR-02:</strong> Every technology evaluation (D14-003) that results in a rejection must have a negative results entry filed within 30 days of the TDR being finalized.</li>
<li><strong>R-NRR-03:</strong> The registry search protocol (Section 4.3) is mandatory before any new formal investigation or technology evaluation begins. The search must be documented in the pre-registration document.</li>
<li><strong>R-NRR-04:</strong> Negative results entries must not be deleted, modified retrospectively, or made less visible. They are append-only records. If subsequent work reveals that a negative result was incorrect (the approach actually works under conditions not tested), a new positive-result entry is created and the negative result entry is updated with a forward reference, but the original entry remains.</li>
<li><strong>R-NRR-05:</strong> The root cause analysis field (Section 4.2) is mandatory. "It did not work" is not a root cause. The entry must explain, to the extent determinable, why it did not work. If the root cause is unknown, the entry must state that explicitly and identify what further investigation would be needed to determine it.</li>
<li><strong>R-NRR-06:</strong> The registry must be stored in plain text format, searchable without specialized tools, and compliant with the 50-year continuity rules. The registry must be usable by a future operator with nothing more than a text editor and the human ability to read.</li>
<li><strong>R-NRR-07:</strong> Negative results must be entered in the knowledge registry (D14-004) under Category 6 (Negative Knowledge) with the appropriate confidence level. The negative results registry and the knowledge registry are complementary, not redundant -- the knowledge registry provides classification and relationships; the negative results registry provides detailed documentation.</li>
</ul>
<h2 id="6-failure-modes-3">6. Failure Modes</h2>
<ul>
<li><strong>Registry neglect.</strong> Negative results are not entered in the registry because the operator considers them unimportant or because the 30-day deadline passes without action. Over time, the registry becomes sparse and unreliable. Mitigation: R-NRR-01 and R-NRR-02 make filing mandatory. The annual classification audit (D14-004, R-KCT-05) cross-checks experiment logs against registry entries to identify missing entries.</li>
<li><strong>Shallow root cause analysis.</strong> Entries are filed but the root cause analysis is superficial -- "it did not work" or "performance was insufficient." These entries are nearly useless for preventing repeated failures because they do not explain why the failure occurred. Mitigation: R-NRR-05 requires substantive root cause analysis. The annual review should flag entries with inadequate root cause analysis for revision.</li>
<li><strong>Search protocol bypass.</strong> The researcher begins a new investigation without searching the registry, either because they consider it unnecessary or because they forgot. The investigation may repeat prior work. Mitigation: R-NRR-03 makes the search mandatory and requires documentation in the pre-registration. A pre-registration that does not include registry search documentation is incomplete.</li>
<li><strong>Registry bloat.</strong> Over decades, the registry becomes so large that searching it is impractical. Mitigation: the annual consolidation process (Section 4.4). The relevance review marks obsolete entries. The category structure of D14-004 provides filtering. If the registry exceeds a manageable size, an index by topic should be created.</li>
<li><strong>False closure.</strong> An approach is recorded as a negative result and future researchers avoid it, even though changed conditions (new hardware, new software, new techniques) might make it viable. Mitigation: the "Recommendations for future work" field (Section 4.2) explicitly addresses this. Entries should distinguish between fundamental failures and conditional failures. The conditions field records what was true when the failure occurred, allowing future researchers to assess whether those conditions still apply.</li>
<li><strong>Psychological avoidance.</strong> The operator simply does not want to document failures because it feels demoralizing. Mitigation: cultural reinforcement through the Commentary Section and through institutional norms established in D14-001. Negative results documentation is framed as an act of service to the future, not as self-flagellation.</li>
</ul>
<h2 id="7-recovery-procedures-3">7. Recovery Procedures</h2>
<ol>
<li><strong>If the registry has been neglected:</strong> Conduct a registry reconstruction sprint. Review all experiment logs (D14-002) and Technology Decision Records (D14-003) from the neglected period. Identify all negative results that should have been entered. Create entries for each, using the available documentation. Accept that some entries will be incomplete due to faded memory or insufficient records -- incomplete entries are better than no entries.</li>
<li><strong>If root cause analyses are shallow:</strong> Review all entries flagged as having insufficient root cause analysis. For recent entries, re-examine the evidence and attempt a deeper analysis. For older entries where re-analysis is not possible, annotate the entry with "Root cause analysis limited by available evidence as of [date]" and document what further investigation would be needed.</li>
<li><strong>If the search protocol has been routinely bypassed:</strong> Conduct a retroactive search for all investigations initiated without registry searches. Check whether any of those investigations repeated prior failures. If so, document the duplication and the cost. Use the concrete examples to reinforce the value of the search protocol. Going forward, treat the search documentation as a gating requirement for pre-registration approval.</li>
<li><strong>If the registry has become bloated and unsearchable:</strong> Create a topic index. Group entries by subject area, technology domain, and failure type. Mark entries whose conditions no longer apply as "historically relevant only." Consider creating a "top 20" list of the most important negative results that every researcher should be aware of.</li>
<li><strong>If negative results documentation has been psychologically avoided:</strong> Acknowledge the avoidance honestly in the Commentary Section. Commit to entering the next three negative results within one week of occurrence. Build momentum. The hardest part is starting.</li>
</ol>
<h2 id="8-evolution-path-3">8. Evolution Path</h2>
<ul>
<li><strong>Years 0-5:</strong> The registry is small. Most entries are from technology evaluations and initial system configuration experiments. The registry search protocol may feel unnecessary because the operator remembers all the failures. Follow it anyway -- the habit matters more than the current utility.</li>
<li><strong>Years 5-15:</strong> The registry begins to earn its keep. The operator encounters a question they investigated before but had forgotten. The registry reminds them. This is the moment the registry transitions from a discipline to a tool. The accumulation of entries starts revealing patterns -- certain categories of approach that consistently fail, certain conditions that consistently cause problems.</li>
<li><strong>Years 15-30:</strong> If succession occurs, the registry is one of the most valuable resources the predecessor can leave. The successor, facing an institution they did not build, needs to know not just what works but what has been tried and does not work. Without the registry, the successor will spend years rediscovering the predecessor's failures.</li>
<li><strong>Years 30-50+:</strong> The registry is a multi-decade record of institutional learning from failure. It contains wisdom that no single person remembers -- the accumulated knowledge of what does not work, why, and under what conditions. It is, in a sense, the institution's immune memory: the record of every pathogen it has encountered and defeated.</li>
</ul>
<h2 id="9-commentary-section-3">9. Commentary Section</h2>
<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong> I am going to fail at things. I am going to try configurations that do not work, evaluate technologies that turn out to be unsuitable, run experiments that disprove my hypotheses, and pursue lines of inquiry that lead nowhere. This is not pessimism. It is realism. The question is not whether I will fail but whether my failures will be wasted.</p>
<p>If I record them -- honestly, with root cause analysis, with context, with recommendations -- then my failures are converted from waste into knowledge. They become the institution's scar tissue: not pretty, but tougher than what was there before.</p>
<p>The hardest entries to write will be the ones about things I really wanted to work. The backup strategy I was proud of but that failed under load. The technology I evangelized but that turned out to be unsuitable. The approach I invested weeks in before discovering it was fundamentally flawed. Those entries will sting. I will write them anyway. The person who needs to read them -- whether that is future-me or a successor -- deserves the truth, not a curated highlight reel.</p>
<h2 id="10-references-3">10. References</h2>
<ul>
<li>ETH-001 -- Ethical Foundations (Principle 2: Integrity Over Convenience; Principle 6: Honest Accounting of Limitations)</li>
<li>CON-001 -- The Founding Mandate (knowledge preservation, institutional continuity)</li>
<li>D14-001 -- Research Philosophy (R-RES-02: negative result documentation, Section 4.2: negative result preservation)</li>
<li>D14-002 -- Experimental Design for Solo Researchers (experiment log format, pre-registration)</li>
<li>D14-003 -- Technology Evaluation Framework (Technology Decision Records, rejected technology documentation)</li>
<li>D14-004 -- Knowledge Classification and Taxonomy (Category 6: Negative Knowledge, confidence levels)</li>
<li>D14-006 -- Multi-Generational Research Continuity (negative results as succession resource)</li>
<li>META-00-ART-001 -- Stage 1 Meta-Framework (50-year continuity rules)</li>
</ul>
<hr/>
<hr/>
<h1 id="d14-006-multi-generational-research-continuity">D14-006 -- Multi-Generational Research Continuity</h1>
<p><strong>Document ID:</strong> D14-006 <strong>Domain:</strong> 14 -- Research & Theory <strong>Version:</strong> 1.0.0 <strong>Date:</strong> 2026-02-16 <strong>Status:</strong> Ratified <strong>Depends On:</strong> ETH-001, CON-001, GOV-001, OPS-001, D14-001, D14-002, D14-003, D14-004, D14-005, D13-001, D11-001 <strong>Depended Upon By:</strong> All Domain 14 articles involving long-term research programs. Referenced by Domain 13 (Evolution) for continuity of research-informed evolution decisions. Referenced by Domain 9 (Documentation & Knowledge) for knowledge transfer standards.</p>
<hr/>
<h2 id="1-purpose-4">1. Purpose</h2>
<p>This article addresses the most difficult challenge in the research domain: how to maintain research programs across the transition from one operator to the next. Everything else in Domain 14 -- experimental design, technology evaluation, knowledge classification, negative results -- exists within the working life of a single operator. This article addresses the boundary between operators, the moment when one person's research practice must become another person's inheritance.</p>
<p>The challenge is not merely logistical. It is not enough to hand over a well-organized archive (though that is necessary). The challenge is epistemological: how does a successor operator understand not just what was known, but why it was investigated, what questions were left open, what approaches were abandoned and why, and what the predecessor's unwritten intuitions and suspicions were? How does the successor distinguish between knowledge that the predecessor was confident about and knowledge that the predecessor accepted provisionally because they ran out of time to investigate further?</p>
<p>CON-001 declares that this institution must be transferable through documentation alone. D14-006 is the research domain's answer to that declaration. It defines the systems that make research continuity possible across the most disruptive event a single-operator institution can face: the replacement of the single operator.</p>
<h2 id="2-scope-4">2. Scope</h2>
<p><strong>In scope:</strong></p>
<ul>
<li>The research handoff protocol: what must be documented, when, and in what format.</li>
<li>Research state documentation: how to capture the current state of all active and suspended research programs.</li>
<li>Open questions registry: how to document questions that remain unanswered.</li>
<li>Abandoned leads documentation: how to record lines of inquiry that were started but not completed, and why they were abandoned.</li>
<li>The research continuity audit: a periodic verification that the research state is documented well enough for handoff.</li>
<li>Successor orientation: how a new operator gets up to speed on the research program.</li>
<li>Long-term research programs: how to design and maintain investigations that span years or decades.</li>
</ul>
<p><strong>Out of scope:</strong></p>
<ul>
<li>General succession planning (Domain 9, Domain 13, GOV-001).</li>
<li>Operational handoff procedures (OPS-001, Domain 12 for emergency succession).</li>
<li>Physical asset transfer (D11-001, ADM-series).</li>
<li>Governance transition (GOV-001).</li>
</ul>
<h2 id="3-background-4">3. Background</h2>
<h3 id="31-the-successors-dilemma">3.1 The Successor's Dilemma</h3>
<p>Imagine a successor operator arriving at this institution for the first time. They have access to all the documentation. They can read the charter, the policies, the operational procedures. They can follow the maintenance checklists and the backup schedules. But when they encounter the research domain, they face a different kind of challenge.</p>
<p>The predecessor was in the middle of evaluating a new storage technology. The evaluation was 70% complete. The preliminary results were promising but inconclusive. The predecessor had a hunch -- based on years of experience with similar technologies -- that a specific failure mode might emerge under sustained load, but they had not yet designed the test. None of this was in the TDR because the evaluation was not finished.</p>
<p>The successor now has three options, all bad. They can complete the evaluation without the predecessor's context, potentially missing the suspected failure mode. They can abandon the evaluation and start fresh, wasting all prior work. Or they can try to reconstruct the predecessor's thinking from incomplete records, which is possible only if the predecessor documented their in-progress research state.</p>
<p>This article ensures that the third option is available and reliable.</p>
<h3 id="32-research-state-as-institutional-infrastructure">3.2 Research State as Institutional Infrastructure</h3>
<p>Most institutions treat research state as ephemeral -- it exists in the researcher's head, in their lab notebook, in their half-finished draft papers. When the researcher leaves, the state evaporates. This is acceptable in a research university where multiple researchers work in overlapping areas and institutional knowledge is distributed. It is not acceptable in a single-operator institution where all research knowledge is concentrated in one mind.</p>
<p>This article treats research state as institutional infrastructure, subject to the same documentation and preservation requirements as any other critical institutional asset. The state of every active research program, every open question, and every abandoned lead must be documented in a form that survives the operator's departure.</p>
<h3 id="33-the-living-research-state-document">3.3 The Living Research State Document</h3>
<p>The key mechanism of research continuity is the Living Research State Document (LRSD) -- a continuously maintained document that captures the current state of all research activity. The LRSD is not a summary written at the end of a research program. It is updated as research progresses, capturing the current state of understanding, the current hypotheses, the current plan, and -- critically -- the current uncertainties and intuitions.</p>
<p>The LRSD is the research equivalent of a ship's log. It records not just what has been decided and done, but what is being considered, what seems promising, what seems suspicious, and what the researcher plans to do next. If the researcher is replaced tomorrow, the LRSD tells the successor where they are in the journey.</p>
<h2 id="4-system-model-4">4. System Model</h2>
<h3 id="41-the-living-research-state-document-lrsd">4.1 The Living Research State Document (LRSD)</h3>
<p>The LRSD is a plain-text document, updated at least monthly, containing the following sections:</p>
<p><strong>Section 1: Active Research Programs.</strong> For each active research program or investigation:</p>
<ul>
<li>Program ID and title.</li>
<li>Start date and expected duration.</li>
<li>Current phase (formulation, pre-registration, execution, analysis, classification).</li>
<li>Current status summary: what has been done, what has been found so far.</li>
<li>Current hypothesis and how it has evolved since the program began.</li>
<li>Current plan: what the next steps are, in what order.</li>
<li>Current concerns: what the researcher is worried about, what might go wrong, what does not feel right. This is the section where intuition and informal suspicion are recorded. It is explicitly permitted -- and encouraged -- to record hunches that are not yet supported by evidence, as long as they are labeled as such.</li>
<li>Dependencies: what other research or operational activities this program depends on.</li>
<li>Resources: what equipment, time, and materials the program requires.</li>
</ul>
<p><strong>Section 2: Open Questions.</strong> A registry of questions that the institution has identified as important but has not yet investigated, or has investigated inconclusively:</p>
<ul>
<li>Question ID and statement.</li>
<li>Date identified.</li>
<li>Why it matters: what institutional decision or understanding depends on the answer.</li>
<li>Current state of knowledge: what is known, what is suspected, what is unknown.</li>
<li>Barriers to investigation: why the question has not been answered yet (lack of time, lack of equipment, prerequisite knowledge missing, etc.).</li>
<li>Priority: high (answer needed within 1 year), medium (answer needed within 3 years), low (answer would be valuable but is not urgent).</li>
</ul>
<p><strong>Section 3: Abandoned Leads.</strong> A registry of lines of inquiry that were started but discontinued:</p>
<ul>
<li>Lead ID and description.</li>
<li>Date started and date abandoned.</li>
<li>Reason for abandonment: why the lead was discontinued. The reasons are classified as: resource constraint (could not afford the time or materials), dead end (investigation revealed the question was not meaningful or answerable), superseded (a better approach was found), or deferred (still potentially valuable but deprioritized).</li>
<li>What was learned before abandonment: even incomplete investigations typically produce some knowledge. Record it.</li>
<li>Conditions for revival: under what circumstances should this lead be revisited? Is there a specific trigger (new technology, new equipment, more time) that would make the investigation viable?</li>
<li>Cross-reference to negative results entry (D14-005) if applicable.</li>
</ul>
<p><strong>Section 4: Research Environment.</strong> A snapshot of the current research infrastructure:</p>
<ul>
<li>Available test equipment and its condition.</li>
<li>Current software tools used for research.</li>
<li>Current limitations and known gaps in research capability.</li>
<li>Planned improvements to research infrastructure.</li>
</ul>
<p><strong>Section 5: Researcher's Notes.</strong> An unstructured section where the operator records observations, speculations, patterns they have noticed, and connections they suspect but have not validated. This section is explicitly informal. Its purpose is to capture the kind of knowledge that is usually lost when a researcher leaves -- the tacit understanding that is too preliminary for formal documentation but too valuable to discard.</p>
<h3 id="42-the-research-handoff-protocol">4.2 The Research Handoff Protocol</h3>
<p>When operator succession occurs, the research handoff follows a defined protocol:</p>
<p><strong>Phase 1: LRSD Finalization (pre-succession).</strong> The departing operator updates the LRSD to reflect the current state of all research activity as of the handoff date. Each active program receives a "handoff summary" addendum that provides the departing operator's honest assessment of the program's viability, the quality of the evidence so far, and their personal recommendation for whether the successor should continue, modify, or abandon the program.</p>
<p><strong>Phase 2: Archive Verification (pre-succession).</strong> The departing operator verifies that all research artifacts -- experiment logs, TDRs, negative results entries, knowledge registry entries, raw data -- are present, organized, and findable. Any gaps in the archive are documented. This verification is conducted using the research continuity audit checklist (Section 4.3).</p>
<p><strong>Phase 3: Orientation (post-succession).</strong> The successor reads the LRSD in its entirety. They may -- and should -- annotate it with questions, disagreements, and alternative perspectives. The LRSD is designed to be the starting point for the successor's understanding, not a set of orders. The successor is free to continue, modify, or abandon any research program based on their own judgment, but they must document their reasoning for departures from the predecessor's plan.</p>
<p><strong>Phase 4: Confirmation (post-succession).</strong> Within 90 days of assuming responsibility, the successor files a Research Continuity Confirmation in the research archive. This document records: which active programs they intend to continue, which they intend to modify (and how), which they intend to suspend or abandon (and why), which open questions they consider priorities, and what new research directions they plan to pursue. This confirmation is not a commitment that binds the successor permanently -- it is a snapshot of their initial assessment, subject to revision as they gain experience.</p>
<h3 id="43-the-research-continuity-audit">4.3 The Research Continuity Audit</h3>
<p>The research continuity audit is a periodic check, conducted annually, that verifies the institution's research state is documented well enough for handoff at any time. The audit is not conducted in anticipation of succession. It is conducted routinely, because the best time to prepare for succession is before you know it is coming.</p>
<p>The audit checklist:</p>
<ol>
<li>Is the LRSD current? Has it been updated within the last 30 days?</li>
<li>Are all active research programs documented in the LRSD with current status?</li>
<li>Does each active program have a current plan with next steps?</li>
<li>Are all experiment logs (D14-002) complete and filed in the research archive?</li>
<li>Are all TDRs (D14-003) complete and filed?</li>
<li>Is the negative results registry (D14-005) current? Are there experiments or evaluations that produced negative results but lack registry entries?</li>
<li>Is the knowledge registry (D14-004) current? Are there classified knowledge items that have not been entered?</li>
<li>Are all open questions documented in the LRSD?</li>
<li>Are all abandoned leads documented in the LRSD with reasons and conditions for revival?</li>
<li>Is the Researcher's Notes section current? Has the operator recorded their current thinking, hunches, and informal observations?</li>
<li>Could a competent successor, reading only the documentation, understand the state of every research program well enough to make informed decisions about continuation?</li>
</ol>
<p>The last question is the critical one. It is subjective, and the operator will be biased toward answering "yes." The mitigation is documentation replication (D14-001, Section 4.3): the operator selects one active research program and attempts to understand its state from the documentation alone, without relying on memory. If the documentation is insufficient, it is improved.</p>
<h3 id="44-long-term-research-programs">4.4 Long-Term Research Programs</h3>
<p>Some investigations span years or decades. Monitoring hardware degradation over time. Tracking the long-term stability of a storage format. Assessing whether a particular technology's community remains healthy over a full product generation. These long-term programs require specific continuity practices:</p>
<p><strong>Milestone documentation.</strong> Long-term programs define milestones -- points at which the accumulated data is analyzed and interim conclusions are drawn. Each milestone produces a formal report that is filed in the research archive and summarized in the LRSD. If the operator changes between milestones, the milestone reports provide a structured history of the program's progress.</p>
<p><strong>Data continuity.</strong> The raw data from long-term programs must be stored in formats that will remain readable for the duration of the program. Per the 50-year continuity rules, this means plain text, CSV, or other simple structured formats. Data stored in proprietary formats that may become unreadable is a continuity risk.</p>
<p><strong>Hypothesis evolution tracking.</strong> Over a long-term program, the initial hypothesis may evolve substantially as data accumulates. The LRSD tracks this evolution, recording not just the current hypothesis but the history of how it changed and what evidence prompted each change. This evolution record is essential for a successor who needs to understand not just what is currently believed but how the understanding developed.</p>
<p><strong>Abandonment criteria.</strong> Long-term programs define, at inception, the conditions under which they should be abandoned. Without explicit abandonment criteria, long-term programs tend to persist indefinitely on institutional inertia, consuming resources without producing value. The criteria should be revisited at each milestone.</p>
<h2 id="5-rules-constraints-4">5. Rules & Constraints</h2>
<ul>
<li><strong>R-MRC-01:</strong> The Living Research State Document must be updated at least monthly. An LRSD that has not been updated in more than 60 days triggers an automatic audit requirement.</li>
<li><strong>R-MRC-02:</strong> The research continuity audit (Section 4.3) must be conducted annually, with results documented in the research archive. The audit may not be deferred for more than one quarter.</li>
<li><strong>R-MRC-03:</strong> Upon succession, the research handoff protocol (Section 4.2) is mandatory. The successor must file a Research Continuity Confirmation within 90 days of assuming responsibility.</li>
<li><strong>R-MRC-04:</strong> All research artifacts (experiment logs, TDRs, negative results entries, knowledge registry entries, raw data, LRSD versions) must be preserved in the research archive in formats compliant with the 50-year continuity rules.</li>
<li><strong>R-MRC-05:</strong> Long-term research programs (expected duration exceeding two years) must define milestones, data continuity requirements, hypothesis evolution tracking, and abandonment criteria at inception. These definitions are filed in the research archive and referenced in the LRSD.</li>
<li><strong>R-MRC-06:</strong> The Researcher's Notes section of the LRSD is explicitly permitted to contain informal, speculative, and subjective content. The operator must not self-censor this section out of concern that informal observations are not "rigorous enough." The purpose of this section is to capture exactly the kind of knowledge that is lost when only formal results are documented.</li>
<li><strong>R-MRC-07:</strong> Departing operators must complete LRSD Finalization and Archive Verification (Phases 1 and 2 of the handoff protocol) before succession takes effect. If succession is unplanned (emergency, incapacitation), the successor conducts the archive verification and documents any gaps discovered.</li>
</ul>
<h2 id="6-failure-modes-4">6. Failure Modes</h2>
<ul>
<li><strong>LRSD staleness.</strong> The LRSD is not updated regularly and drifts out of sync with actual research activity. If succession occurs during a stale period, the successor inherits an inaccurate picture of the research state. Mitigation: R-MRC-01's monthly update requirement. The annual audit (R-MRC-02) checks LRSD currency.</li>
<li><strong>Formality suppression.</strong> The operator keeps the formal sections of the LRSD current but neglects the Researcher's Notes section, losing the informal knowledge that is often the most valuable for a successor. Mitigation: R-MRC-06 explicitly legitimizes informal content. The audit checklist (Section 4.3, item 10) specifically checks for current notes.</li>
<li><strong>Handoff document overload.</strong> The LRSD becomes so long and detailed that a successor cannot absorb it in a reasonable time. Mitigation: the LRSD should be structured for progressive reading. The active programs section provides the immediate context. The open questions and abandoned leads provide deeper history. The researcher's notes provide the most intimate context. A successor can start with the first section and deepen their reading as needed.</li>
<li><strong>Successor divergence without documentation.</strong> The successor departs from the predecessor's research directions without documenting their reasoning. Over time, the institution loses track of why certain programs were continued and others were not. Mitigation: R-MRC-03 requires a Research Continuity Confirmation that documents the successor's decisions.</li>
<li><strong>Long-term program inertia.</strong> A long-term research program persists past its usefulness because no one reviews the abandonment criteria. Mitigation: R-MRC-05 requires abandonment criteria at inception. Milestone reviews include an explicit check against those criteria.</li>
<li><strong>Emergency succession gap.</strong> The operator is incapacitated or departs unexpectedly without completing the handoff protocol. Mitigation: the continuous maintenance of the LRSD (updated monthly, audited annually) means that even without a formal handoff, the successor has a reasonably current snapshot of the research state. The gap between the last LRSD update and the succession event is at most 60 days (per R-MRC-01), and the content of that gap can often be partially reconstructed from operational logs and experiment logs.</li>
</ul>
<h2 id="7-recovery-procedures-4">7. Recovery Procedures</h2>
<ol>
<li><strong>If the LRSD has gone stale:</strong> Conduct an immediate update. For each research program, determine its current state from memory, experiment logs, and operational logs. Update the LRSD. File a note in the Commentary Section acknowledging the period of staleness and any information that may have been lost during that period.</li>
<li><strong>If research continuity audits have been skipped:</strong> Conduct the audit immediately using the checklist in Section 4.3. Accept that some items may reveal significant gaps. Prioritize closing the gaps based on the severity of the consequence if succession occurred today.</li>
<li><strong>If a succession has occurred without a formal handoff:</strong> The successor conducts an archive inventory, identifying all available research artifacts. They construct a best-effort LRSD from the available materials. Gaps are documented explicitly. The successor files a Research Continuity Confirmation that acknowledges the limitations of the informal handoff and identifies areas where the predecessor's knowledge is irrecoverably lost.</li>
<li><strong>If long-term research programs have persisted without milestone reviews:</strong> Conduct milestone reviews for all long-term programs immediately. For each, assess: is the program still producing value? Has the original question been answered (even partially)? Do the abandonment criteria suggest the program should be closed? Document the review findings and make continuation decisions explicitly.</li>
<li><strong>If the Researcher's Notes section has been neglected:</strong> Set aside one hour. Write down everything you currently think, suspect, wonder about, and worry about regarding the institution's research programs. Do not edit for formality. Do not censor speculation. The goal is to externalize the tacit knowledge that has been accumulating in your head. Then commit to adding to this section weekly, even if the additions are brief.</li>
</ol>
<h2 id="8-evolution-path-4">8. Evolution Path</h2>
<ul>
<li><strong>Years 0-5:</strong> The LRSD is new and the operator is the founder. The handoff protocol exists in documentation but has not been tested. The value of the LRSD in this period is primarily as a discipline -- the monthly update forces the operator to reflect on research state and direction. The Researcher's Notes section, if maintained honestly, will become the most valuable section in a decade.</li>
<li><strong>Years 5-15:</strong> The LRSD and associated research artifacts accumulate substance. If a succession occurs in this period, the handoff protocol is tested for the first time. The first succession will almost certainly reveal gaps in the documentation -- things the predecessor took for granted that the successor needs explained. These gaps should be documented and used to improve the LRSD template and the handoff protocol for future successions.</li>
<li><strong>Years 15-30:</strong> The institution may have experienced one or more successions. The LRSD contains layers of research state from different operators, each with their own perspectives, priorities, and notes. The Researcher's Notes section, spanning multiple operators, becomes a conversation across time -- each operator reading the notes of their predecessors and adding their own.</li>
<li><strong>Years 30-50+:</strong> The LRSD, experiment archive, knowledge registry, and negative results registry together form a multi-decade record of the institution's intellectual life. This record is one of the institution's most valuable assets. It contains not just what was known but how it was discovered, not just what worked but what was tried, not just what was decided but what was wondered. The continuity mechanisms defined in this article are what make this record possible.</li>
</ul>
<h2 id="9-commentary-section-4">9. Commentary Section</h2>
<p><em>This section is reserved for dated entries by current and future operators.</em></p>
<p><strong>2026-02-16 -- Founding Entry:</strong> I am writing a succession document for a position that only I hold, for a successor who may not exist for years. This feels premature. It is not.</p>
<p>The truth is that I do not know when succession will happen. It could be planned, decades from now, when I choose to hand off the institution. It could be unplanned, next year, through accident or illness. If I wait until succession is imminent to prepare the research handoff, I will either rush the documentation or leave it undone.</p>
<p>The LRSD is not just a succession tool. It is a thinking tool. The monthly update forces me to step back from the details of individual experiments and ask: where is the research program going? What do I think I know? What am I unsure about? What have I been meaning to investigate but have not gotten to? These are valuable questions regardless of whether a successor ever reads my answers.</p>
<p>The section I am most committed to maintaining is the Researcher's Notes. I know from experience in other contexts that the most valuable knowledge a departing person has is the stuff that never makes it into formal documentation -- the hunches, the suspicions, the patterns they noticed but never proved, the approaches they discarded for reasons they never wrote down. I want to capture as much of that as possible, not because it is rigorous (it is not) but because it is real, and because the person who comes after me will need every advantage I can give them.</p>
<h2 id="10-references-4">10. References</h2>
<ul>
<li>ETH-001 -- Ethical Foundations (Principle 6: Honest Accounting of Limitations)</li>
<li>CON-001 -- The Founding Mandate (transferability through documentation, lifetime operation)</li>
<li>GOV-001 -- Authority Model (succession procedures, decision documentation)</li>
<li>OPS-001 -- Operations Philosophy (documentation-first, operational logging)</li>
<li>D14-001 -- Research Philosophy (knowledge lifecycle, replication, anti-echo-chamber structures)</li>
<li>D14-002 -- Experimental Design for Solo Researchers (experiment log format)</li>
<li>D14-003 -- Technology Evaluation Framework (Technology Decision Records)</li>
<li>D14-004 -- Knowledge Classification and Taxonomy (knowledge registry, confidence levels)</li>
<li>D14-005 -- Negative Results Registry (negative result documentation)</li>
<li>D13-001 -- Evolution Philosophy (research as input to evolution, change management)</li>
<li>D11-001 -- Administration Philosophy (resource management, scheduling)</li>
<li>D9-001 -- Documentation Philosophy (documentation standards, competence levels)</li>
<li>META-00-ART-001 -- Stage 1 Meta-Framework (50-year continuity rules)</li>
</ul>
<hr/>
<p><em>End of Stage 4: Research & Theory Advanced Articles</em></p>
</main>
</div>
<footer class="site-footer">
<div class="footer-inner">
<p>holm.chat Documentation Institution &mdash; Air-Gapped, Off-Grid, Self-Built</p>
<p>Stage 1: Documentation Framework &mdash; Version 1.0.0 &mdash; 2026-02-16</p>
</div>
</footer>
</body>
</html>